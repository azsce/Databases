---
title: "2. chapter 2"
---

# Database System Concepts and Architecture


## Database System Concepts and Architecture

Alright, let's delve into Chapter 2, titled "Database System Concepts and Architecture." This chapter is crucial as it lays down the conceptual and architectural framework for understanding how database systems are designed and how they operate. We'll be looking at the blueprints, so to speak, of these complex systems.

---

## Outline

So, what specific topics will we be navigating in this chapter? Let's review our outline.

First, we'll explore **Data Models and Their Categories**. Think of data models as the languages we use to describe data.
Then, we'll touch upon the **History of Data Models**, to see how these "languages" have evolved over time.
A key distinction we'll make is between **Schemas, Instances, and States**. Understanding these terms is fundamental.
We'll then discuss the **Three-Schema Architecture**, a conceptual model that helps us understand the different levels of a database.
This leads naturally into the concept of **Data Independence**, a very important characteristic that this architecture supports.
Next, we'll look at **DBMS Languages and Interfaces** – how we actually communicate with database systems.
We'll also cover **Database System Utilities and Tools**, which are essential for managing and maintaining databases.
We'll then examine **Centralized and Client-Server Architectures**, looking at how database systems are physically and logically deployed.
And finally, we'll discuss the **Classification of DBMSs**, to understand the different types of database management systems out there.

A packed agenda, but each piece is vital for a comprehensive understanding.

<div class="page-break"></div>

## Data Models

Let's begin with the very concept of a **`Data Model`**.

So, what is a data model?
A **`Data Model`** is essentially a collection of conceptual tools. It's a set of concepts we use to describe three key things:
First, the **`structure`** of a database – how the data is organized, what types of data are stored, and how they relate to each other.
Second, the **`operations`** for manipulating these structures – what actions can we perform on the data, like retrieving it, updating it, or deleting it.
And third, certain **`constraints`** that the database should obey – these are rules that ensure the data remains accurate and consistent.

Now, let's break down the **`Data Model Structure and Constraints`** a bit further.
*   The **`constructs`** are the building blocks used to define the database structure.
*   These constructs typically include basic **`elements`** (like a student's name or age) and their associated **`data types`** (is it text? a number? a date?). They also include groups of these elements, which we might call an **`entity`** (like a STUDENT), a **`record`**, or a **`table`**. And crucially, they define the **`relationships`** among these groups. For example, how a STUDENT entity relates to a COURSE entity.
*   **`Constraints`**, then, specify restrictions on what constitutes valid data. For example, a constraint might state that a student's GPA must be between 0 and 4.0. These constraints are not just suggestions; they *must* be enforced by the DBMS at all times to maintain data integrity.

---

Continuing with our discussion on Data Models...

Let's talk about **`Data Model Operations`**.
*   These operations are the verbs of our data model, if you will. They are used for specifying how we interact with the data – specifically, for database **`retrievals`** (getting data out) and **`updates`** (modifying the data). When we perform these operations, we refer to the constructs of the data model that we just discussed.
*   Operations on the data model can be categorized. We have **`basic model operations`**, which are generic actions like `insert` (add new data), `delete` (remove data), and `update` (change existing data). These are fundamental to any data model.
*   Then, we often have **`user-defined operations`**. These are more specific to the application. For example, an operation like `compute_student_gpa` or `update_inventory`. These are typically built using the basic operations but provide more complex, business-specific functionality.

So, a data model provides not just the blueprint for the data's structure, but also the toolkit for working with that data.

<div class="page-break"></div>

## Categories of Data Models

Data models are not all the same; they can be broadly categorized based on the level of abstraction they provide.

First, we have **`Conceptual (high-level, semantic) data models`**.
*   These models provide concepts that are very close to how users actually perceive and understand data. They focus on the meaning of the data – the semantics.
*   You'll often hear these referred to as **`entity-based`** or **`object-based`** data models, as they often deal with real-world entities and objects and their relationships. Think of an Entity-Relationship diagram, which is a classic example.

Next, at the other end of the spectrum, are **`Physical (low-level, internal) data models`**.
*   These models are concerned with the nitty-gritty details of how data is actually stored in the computer. They describe storage structures, file organizations, access paths like indexes, and so on.
*   These details are usually specified in a rather ad-hoc manner, often found within the design and administration manuals of a specific DBMS. They are very close to the hardware.

Then, bridging the gap between conceptual and physical, we have **`Implementation (representational) data models`**.
*   These models provide concepts that fall somewhere between the high-level conceptual view and the low-level physical details.
*   Crucially, these are the models that are typically used by most commercial DBMS implementations. The most prominent example here is the **`relational data model`**, which forms the basis for systems like Oracle, SQL Server, MySQL, and many others. We'll be spending a lot of time on the relational model in this course.

Finally, there's an important category called **`Self-Describing Data Models`**.
*   What's unique about these models is that they combine the description of the data (the schema or metadata) *with* the data values themselves. The data carries its own description, so to speak.
*   Examples of this include **`XML`** (Extensible Markup Language), **`key-value stores`** (which are very popular in NoSQL databases), and indeed, many **`NoSQL systems`** in general. This approach offers a lot of flexibility.

Understanding these categories helps us appreciate the different ways we can think about and work with data.

<div class="page-break"></div>

## Schemas versus Instances

Now, let's clarify two fundamental concepts in database terminology: **`schema`** and **`instance`**.

First, the **`Database Schema`**:
*   The schema is essentially the **`description`** of a database. It’s the blueprint, the plan, or the structure.
*   It includes detailed descriptions of the database structure (like the tables and their columns in a relational database), the **`data types`** for each piece of information (e.g., `StudentID` is an integer, `Name` is a string of characters), and any **`constraints`** that apply to the data (like `StudentID` must be unique).
*   Think of the schema as the *design* of the database, which is defined at the time the database is created.

Next, we have the **`Schema Diagram`**:
*   This is simply an **`illustrative`** display of the database schema, or at least most aspects of it. It’s a visual representation, often a graphical chart, that helps us understand the structure and relationships within the schema.

And within a schema, we have **`Schema Constructs`**:
*   A schema construct is a **`component`** of the schema or an object defined within the schema.
*   For example, in a university database schema, `STUDENT` would be a schema construct, and `COURSE` would be another. These are the building blocks of the schema.

---

### Example of a Database Schema

Let's look at a visual example of a database schema. This is Figure 2.1, which depicts a schema diagram for a university database, likely the one mentioned as Figure 1.2 in a previous context.

Here we see several schema constructs, represented as boxes with their attributes listed:

*   First, we have **STUDENT**. The attributes, or pieces of information we store for each student, are `Name`, `Student_number`, `Class`, and `Major`.
*   Next, there's **COURSE**, with attributes `Course_name`, `Course_number`, `Credit_hours`, and `Department`.
*   Then we have **PREREQUISITE**. This table links courses to their prerequisites. Its attributes are `Course_number` and `Prerequisite_number`. This implies a relationship: a course can have other courses as prerequisites.
*   Following that is **SECTION**. This represents specific offerings of a course in a particular semester and year. It has attributes `Section_identifier`, `Course_number` (linking back to the COURSE table), `Semester`, `Year`, and `Instructor`.
*   Finally, there's **GRADE_REPORT**. This table stores the grades students receive in specific sections of courses. Its attributes are `Student_number` (linking to STUDENT), `Section_identifier` (linking to SECTION), and `Grade`.

This diagram gives us a clear, high-level overview of the structure of our university database. It shows the different types of information we're storing and the attributes that describe them. The lines between attributes in some tools might represent primary and foreign keys, but here they are just listed. This is the *design*, the *blueprint* of our database.

---

Now, let's contrast the schema with the **`Database State`**:

*   The **`Database State`** refers to the *actual data* that is stored in a database at a **`particular moment in time`**. It’s not the design, but the content. This includes the collection of *all* the data currently in the database.
*   Another term for database state is **`database instance`**, or sometimes an **`occurrence`** or a **`snapshot`**. These terms all emphasize that it's the data as it exists right now.
*   It's also important to note that the term **`instance`** can be applied to individual components of the database. For example, a specific student's information (like "John Doe, ID 123, Class Junior, Major CS") would be a *record instance* or an *entity instance* within the `STUDENT` table. Similarly, all the current rows in the `STUDENT` table would constitute a *table instance*.

So, the schema is the structure, and the state or instance is the actual data residing within that structure at any given time.

<div class="page-break"></div>

## Database Schema vs. Database State

Let's continue to refine our understanding of the distinction between a database schema and a database state.

First, revisiting **`Database State`**:
*   As we've said, this refers to the **`content`** of a database at a specific moment in time. It’s the collection of all data values currently held.

Then, consider the **`Initial Database State`**:
*   This is a special state. It refers to the database state as it exists when it is *initially loaded* into the system. This might be an empty state (just the schema with no data), or it might contain some foundational data.

And a crucial concept is the **`Valid State`**:
*   A database is said to be in a **`valid state`** if all the data currently in it satisfies the structure and all the constraints defined in the database schema. For example, if a schema defines that a `Grade` must be A, B, C, D, or F, then a state containing a `Grade` of 'Z' would be an *invalid* state. The DBMS works to ensure the database always remains in a valid state.

---

Let's emphasize the **`Distinction`** between schema and state:
*   The **`database schema`** is relatively static. It changes very *infrequently*. You design the schema, and then it typically stays the same for long periods. Changes to the schema are significant events, often requiring careful planning.
*   In contrast, the **`database state`** changes constantly. It changes *every time* the database is updated – every time a new student enrolls, a grade is entered, or a course is added. It's dynamic.

There are also some alternative terms you might encounter:
*   The **`Schema`** is sometimes referred to as the **`intension`** of the database. Think of "intention" as the definition or the meaning.
*   And the **`State`** is sometimes referred to as the **`extension`** of the database. Think of "extension" as the set of all tuples or records that satisfy the definition at a point in time.

So, schema is the definition, and state is the current population of data that fits that definition.

---

### Example of a Database Schema

We've seen this schema diagram before (Figure 2.1), but it's good to reiterate. This diagram visually represents the *structure* of our university database.

*   We have the **STUDENT** table with its columns: `Name`, `Student_number`, `Class`, and `Major`.
*   The **COURSE** table with `Course_name`, `Course_number`, `Credit_hours`, and `Department`.
*   The **PREREQUISITE** table, defining relationships between courses via `Course_number` and `Prerequisite_number`.
*   The **SECTION** table, detailing specific course offerings with `Section_identifier`, `Course_number`, `Semester`, `Year`, and `Instructor`.
*   And the **GRADE_REPORT** table, linking students to sections and their grades, with `Student_number`, `Section_identifier`, and `Grade`.

Remember, this is the *schema*. It tells us *how* the data is organized, what attributes are expected, but it doesn't show any actual student names or course details. That comes next, with the database state.

---

### Example of a database state

Now, let's look at an *example of a database state*. This is Figure 1.2, which shows actual data populating the schema we just discussed. This is a snapshot of the database at a particular moment.

Let's examine the tables with their data:

In the **COURSE** table:
*   We see rows like: "Intro to Computer Science" with `Course_number` CS1310, 4 `Credit_hours`, in the `Department` CS.
*   Another is "Data Structures", CS3320, 4 credits, CS department.
*   "Discrete Mathematics", MATH2410, 3 credits, MATH department.
*   And "Database", CS3380, 3 credits, CS department.
This is actual *data* that conforms to the COURSE schema.

Now, look at the **SECTION** table:
*   `Section_identifier` 85 is MATH2410, offered in Fall, year '04, taught by King.
*   `Section_identifier` 92 is CS1310, Fall '04, by Anderson.
*   And so on for other sections, like 102 for CS3320 in Spring '05 by Knuth.
This shows specific instances of course offerings.

Next, the **GRADE_REPORT** table:
*   `Student_number` 17, in `Section_identifier` 112, received a `Grade` of B.
*   The same student 17, in section 119, got a C.
*   `Student_number` 8, in section 85, got an A.
This is the actual academic performance data.

Finally, the **PREREQUISITE** table:
*   For `Course_number` CS3380 (Database), a `Prerequisite_number` is CS3320 (Data Structures).
*   CS3380 also has MATH2410 (Discrete Mathematics) as a prerequisite.
*   And CS3320 (Data Structures) has CS1310 (Intro to Computer Science) as a prerequisite.
This defines the dependencies between courses.

This entire collection of tables filled with data represents the *database state* or *instance* at this moment. If a new student enrolls tomorrow, this state will change. The schema, however, will likely remain the same.

<div class="page-break"></div>

## Three-Schema Architecture

Let's now discuss a very influential concept in database design: the **`Three-Schema Architecture`**.

This architecture was proposed primarily to support two key characteristics of modern Database Management Systems:
*   First, **`Program-data independence`**. This is the idea that application programs should be shielded from changes in how the data is physically stored or even how it's conceptually organized, to a certain extent.
*   Second, the support of **`multiple views`** of the data. Different users or applications may need to see the same underlying data in different ways, tailored to their specific needs.

It's important to note that while this three-schema architecture is a powerful conceptual model, it's *not explicitly implemented* in its exact theoretical form by most commercial DBMS products. However, its principles have been incredibly useful in explaining database system organization and in guiding the design of actual systems. It provides a valuable framework for thinking about database structure.

---

So, what are these three schemas or levels? The Three-Schema Architecture defines DBMS schemas at, unsurprisingly, **`three levels`**:

1.  **`Internal schema`**: This is at the *internal level*. It describes the physical storage structures and access paths used by the database. For example, it would specify how records are stored on disk, what indexes exist to speed up data retrieval, the file organization, and so on.
    *   This level typically uses a **`physical data model`**.

2.  **`Conceptual schema`**: This is at the *conceptual level*. It provides a comprehensive description of the structure and constraints for the *whole database* for a *community of users*. It represents the logical view of the entire database, independent of how it's physically stored. It defines all the entities, attributes, and relationships of interest.
    *   This level typically uses a **`conceptual data model`** (like an ER model) or an **`implementation data model`** (like the relational model).

3.  **`External schemas`**: This is at the *external level*, also known as the *view level*. It describes the various *user views*. Each external schema defines the portion of the database that a particular user or group of users is interested in, and it hides the rest of the database from that user group. There can be many external schemas for a single conceptual schema.
    *   The external schemas usually use the **`same data model`** as the conceptual schema (e.g., if the conceptual schema is relational, the external views are also typically relational views).

This layered approach is key to achieving data independence and supporting multiple views.

---

### The three-schema architecture

Let's look at Figure 2.2, which visually represents this three-schema architecture.

At the top, we have the **`End Users`**. These are the people or applications interacting with the database.

They interact through the **`External Level`**. Here, we see multiple **`External Views`** (labeled `External View`, `...`, `External View`). Each user or user group might have their own specific view of the data. For instance, a registrar might see student academic records, while a financial aid officer might see student financial details. These are tailored perspectives.

These external views are derived from the **`Conceptual Level`**, which contains the single, unified **`Conceptual Schema`**. The conceptual schema represents the entire database's logical structure for the whole community of users. The arrow pointing from the External Views to the Conceptual Schema is labeled **`External/Conceptual Mapping`**. This mapping defines how an external view is derived from the conceptual schema.

Below the conceptual schema is the **`Internal Level`**, which contains the **`Internal Schema`**. The internal schema describes how the conceptual schema is physically implemented – the storage structures, indexes, and so on. The arrow from the Conceptual Schema to the Internal Schema is labeled **`Conceptual/Internal Mapping`**. This mapping defines how the conceptual structures are translated into physical storage.

Finally, at the very bottom, the internal schema maps to the **`Stored Database`**, represented by several disk symbols. This is where the data actually resides.

So, requests from users come in via an external view, are mapped to the conceptual schema, then mapped to the internal schema, and finally access the stored data. Conversely, data retrieved from storage flows back up through these mappings to be presented in the user's external view. This separation of levels is fundamental.

---

Continuing with the Three-Schema Architecture:

An essential aspect of this architecture is the need for **`mappings`** among the schema levels. These mappings are crucial for transforming requests and data as they move between the levels.

*   When application **`programs`** execute, they typically refer to an **`external schema`** (their specific view of the data). The DBMS then uses the mappings to translate these requests, first to the **`conceptual schema`** and then to the **`internal schema`** for actual execution against the stored database.
*   Conversely, when **`data is extracted`** from the internal DBMS level (the physical storage), it needs to be reformatted to match the user's **`external view`**. For example, if a user submits an SQL query (which operates on their external view), the results retrieved from the physical files might need to be restructured, columns reordered, or calculations performed to present it in the format defined by that external view, perhaps for display in a Web page.

These mappings are what enable the different levels to be somewhat independent of each other, which is a key benefit we'll discuss next.

---

## Data Independence (Slide 2-18)

The Three-Schema Architecture we just discussed directly enables a very important concept called **`Data Independence`**. This is a crucial characteristic of well-designed database systems. There are two primary types of data independence:

First, **`Logical Data Independence`**:
*   This refers to the **`capacity to change the conceptual schema without having to change the external schemas`** and, importantly, their associated application programs.
*   What does this mean? It means we can modify the overall logical structure of the entire database – perhaps add a new entity, add an attribute to an existing entity, or change a relationship – without necessarily breaking existing applications that use their own external views, *as long as those views can still be derived from the modified conceptual schema*. This provides a significant degree of flexibility in evolving the database design without forcing widespread application rewrites.

Second, **`Physical Data Independence`**:
*   This refers to the **`capacity to change the internal schema without having to change the conceptual schema`**.
*   This means we can alter how the data is physically stored – for example, we might change the file organization for a table, add or drop an index, or move data to a different storage device – all to improve performance or manage storage better. Crucially, these changes at the internal level should *not* require any changes to the conceptual schema or, by extension, the external schemas and application programs.
*   The slide gives a good example: **`the internal schema may be changed when certain file structures are reorganized or new indexes are created to improve database performance`**. Physical data independence ensures that such tuning activities, which are common for DBAs, don't force a rewrite of applications that access the data logically.

Both types of data independence are highly desirable, as they reduce the maintenance effort and protect investments in application software when the database inevitably evolves over time.

---

## Data Independence (continued) (Slide 2-19)

Let's elaborate further on the implications of Data Independence.

■ **`When a schema at a lower level is changed`** – for example, if the internal schema is modified to improve performance, or if the conceptual schema is altered (perhaps by adding a new table or attribute) – **`only the mappings between this schema and higher-level schemas need to be changed in a DBMS that fully supports data independence.`**
*   So, if the internal schema changes, the mapping between the internal and conceptual schemas might need an update. If the conceptual schema changes in a way that still allows existing external views to be derived, then the mappings between the external and conceptual schemas might need adjustment.

■ The crucial point here is that **`The higher-level schemas themselves are unchanged.`**
*   For example, if the internal schema changes, the conceptual schema ideally remains the same. If the conceptual schema changes, the external schemas (the views that applications use) can often remain the same, provided they can still be logically derived.

■ **`Hence, the application programs need not be changed since they refer to the external schemas.`**
*   This is the real power of data independence. Because applications are programmed against stable external schemas, changes happening "underneath" (at the conceptual or internal levels) do not necessarily propagate up to force modifications in the application code. This isolation significantly reduces the ripple effect of database modifications, saving considerable time, effort, and resources in software maintenance.

This ability to evolve parts of the database system without breaking other parts is a cornerstone of modern database management.

<div class="page-break"></div>

## DBMS Languages (Slide 2-20)

Now that we understand the architecture and the importance of data independence, let's talk about how we actually *communicate* with a Database Management System. To do this, we use specialized **`DBMS Languages`**.

There are two primary categories of these languages:

■ First, the **`Data Definition Language (DDL)`**.
*   As its name strongly suggests, the DDL is used to *define* the database. This includes specifying the structure of the database (the schema), creating tables, defining the data types for each attribute within those tables, and establishing any constraints that the data must adhere to. Think of DDL as the language used by architects to design a building.

■ Second, the **`Data Manipulation Language (DML)`**.
*   Once the database structure is defined using DDL, the DML is used to *manipulate* the data within that structure. This involves operations like:
    *   Retrieving data (querying).
    *   Inserting new data.
    *   Deleting existing data.
    *   Modifying (updating) existing data.
    *   If DDL builds the house, DML is used to move furniture in and out, redecorate, and find things.

Now, DMLs themselves can be further categorized based on their level of abstraction and how they are used:

*   **`High-Level or Non-procedural Languages`**:
    *   These languages allow users to specify *what* data they want, rather than detailing *how* to retrieve it. The DBMS figures out the optimal procedure.
    *   The most prominent example here is the relational language **`SQL (Structured Query Language)`**.
    *   SQL, and similar high-level languages, **`May be used in a standalone way`** (e.g., an analyst typing SQL queries directly into a database tool) or they **`may be embedded in a programming language`** (e.g., a Java application making SQL calls).
*   **`Low Level or Procedural Languages`**:
    *   These languages require the programmer to specify the step-by-step procedure for accessing data, often working with one record at a time.
    *   They offer finer control but are generally more complex to use.
    *   These languages **`must be embedded in a programming language`**; they are not typically used in a standalone interactive mode for general querying.

We will explore DDL and DML in more detail.

---

## DBMS Languages (Slide 2-21, focuses on DDL)

Let's focus more specifically on the **`Data Definition Language (DDL)`**.

■ **`Used by the DBA and database designers to specify the conceptual schema of a database.`**
*   The primary users of DDL are Database Administrators (DBAs) and database designers. Their role is to define the overall logical structure of the database – the conceptual schema. This involves creating tables, defining columns, setting primary and foreign keys, and specifying constraints.

■ **`In many DBMSs, the DDL is also used to define internal and external schemas (views).`**
*   While the DDL's core function is to define the conceptual schema, in most modern DBMSs, extensions to the DDL or specific DDL commands are also used to:
    *   Define aspects of the *internal schema* (e.g., creating indexes, specifying storage parameters for tables).
    *   Define *external schemas*, which in the context of SQL, are known as **`views`**. A view is a virtual table defined by a query, providing a customized perspective of the data.

■ **`In some DBMSs, separate storage definition language (SDL) and view definition language (VDL) are used to define internal and external schemas.`**
*   This was more common in older or specialized systems. The idea was to have distinct languages for distinct purposes:
    *   **`SDL (Storage Definition Language)`** would be used exclusively for defining the physical storage aspects (the internal schema).
    *   **`VDL (View Definition Language)`** would be used exclusively for defining user views (the external schemas).
*   The slide notes that **`SDL is typically realized via DBMS commands provided to the DBA and database designers`**. So, even if not a completely separate "language," there are specific commands or syntax within the broader DDL framework that address storage definition.

In essence, DDL is the language we use to build and modify the very structure of our database.

---

## DBMS Languages (Slide 2-22, focuses on DML)

Now let's turn our attention to the **`Data Manipulation Language (DML)`**.

■ **`Used to specify database retrievals and updates`**.
*   Once the database schema is defined with DDL, DML is the language used to interact with the *data* itself. This includes fetching data (querying), adding new records, removing records, and changing existing records.

■ **`DML commands (data sublanguage) can be embedded in a general-purpose programming language (host language), such as COBOL, C, C++, or Java.`**
*   A very common way to use DML is to embed DML statements (like SQL queries) directly into the code of an application written in a standard programming language. This allows the application to dynamically interact with the database to store and retrieve information as part of its business logic.
*   The slide also mentions that **`A library of functions can also be provided to access the DBMS from a programming language`**. This refers to Application Programming Interfaces (APIs) like JDBC for Java or ODBC for C/C++. Instead of direct embedding, the host language calls functions from these libraries to execute DML operations.

■ **`Alternatively, stand-alone DML commands can be applied directly (called a query language).`**
*   DML, especially SQL, can also be used interactively. A user can type SQL queries directly into a DBMS tool (like SQL*Plus for Oracle, or psql for PostgreSQL) and get immediate results. In this context, particularly for data retrieval, the DML is often referred to specifically as a **`query language`**.

So, DML is our tool for working with the actual content of the database.

---

## Types of DML (Slide 2-23)

We've touched on this, but it's worth reiterating the two main **`Types of DML`** based on their procedurality.

■ **`High Level or Non-procedural Language`**:
*   **`For example, the SQL relational language`**. SQL is the quintessential example.
*   These languages **`Are "set"-oriented`**. This means they operate on entire sets of records (or tuples, in relational terms) at once, rather than processing data one record at a time. When you write an SQL query, you're typically describing a set of data you want.
*   Crucially, they **`specify what data to retrieve rather than how to retrieve it.`** The user declares their intent, and the DBMS query optimizer is responsible for figuring out the most efficient step-by-step procedure to satisfy that request.
*   Because of this declarative nature, they are **`Also called declarative languages.`**

■ **`Low Level or Procedural Language`**:
*   These languages, in contrast, **`Retrieve data one record-at-a-time`**.
*   To process multiple records, the programmer needs to use programming **`Constructs such as looping`**. They also typically need to explicitly manage **`positioning pointers`** (often called cursors) to keep track of the current record they are working with within a set of records.
*   This approach gives more explicit, fine-grained control over the data access process but requires more detailed programming effort and understanding of the data's physical layout or access paths.

Most modern database application development heavily leverages high-level, non-procedural DMLs like SQL due to their ease of use, power, and the benefits of query optimization handled by the DBMS.

<div class="page-break"></div>

## DBMS Interfaces (Slide 2-24)

Database Management Systems provide various **`interfaces`** to allow different types of users and programs to interact with the database. Let's look at some common types:

■ **`Stand-alone query language interfaces`**:
*   These allow users to directly type in queries in the DBMS's query language, typically SQL.
*   The slide gives an **`Example: Entering SQL queries at the DBMS interactive SQL interface (e.g. SQL*Plus in ORACLE)`**. Other databases have similar command-line or graphical tools for interactive SQL execution. This is often used by DBAs, developers for testing, or data analysts for ad-hoc querying.

■ **`Programmer interfaces for embedding DML in programming languages`**:
*   These are crucial for software developers. They provide the mechanisms to include DML commands (like SQL statements) within the code of application programs written in general-purpose languages like Java, C++, Python, etc. This allows applications to dynamically interact with the database as part of their core functionality. We'll explore the different approaches for this shortly.

■ **`User-friendly interfaces`**:
*   These are designed for end-users who may not be database experts or programmers. They aim to simplify interaction and make the database accessible to a wider audience.
*   Examples include:
    *   **`Menu-based`** interfaces, where users select options from menus to navigate and perform actions.
    *   **`Forms-based`** interfaces, where users fill in data into on-screen forms, much like paper forms, which is very common for data entry tasks.
    *   **`Graphics-based`** interfaces, which might use icons, drag-and-drop functionality, or visual query builders to help users formulate requests without writing code.
    *   And so on. The goal is to make the database interaction intuitive for the target user.

■ **`Mobile Interfaces:interfaces allowing users to perform transactions using mobile apps`**:
*   With the ubiquity of smartphones and tablets, specialized interfaces delivered through **`mobile apps`** are now a very common way for users to interact with backend databases. These interfaces are optimized for smaller screens, touch input, and often for specific, streamlined tasks.

The choice of interface depends heavily on the user's role, their technical skill level, and the specific tasks they need to perform with the database.

---

## DBMS Programming Language Interfaces (Slide 2-25)

Let's delve deeper into the **`Programmer interfaces for embedding DML in a programming languages`**. These are the toolkits that software developers use to make their applications talk to databases. There are several common approaches:

*   **`Embedded Approach`**:
    *   In this approach, DML statements (typically SQL) are directly written *within* the source code of a host programming language.
    *   For example, we have **`embedded SQL (for C, C++, etc.)`**. A special precompiler processes these embedded SQL statements before the main compiler for the host language.
    *   **`SQLJ`** is a specific standard for embedding SQL directly into Java code, offering a more static and type-safe way of handling SQL than JDBC for certain scenarios.

*   **`Procedure Call Approach`**: (This is also often referred to as an API-based approach).
    *   Instead of direct embedding, the programming language uses a library of functions – an Application Programming Interface (API) – to interact with the database. The application makes calls to these API functions to connect to the database, execute SQL statements, fetch results, handle errors, etc.
    *   Prominent examples include:
        *   **`JDBC (Java Database Connectivity) for Java`**. This is the standard API for Java applications to interact with virtually any relational database.
        *   **`ODBC (Open Database Connectivity)`** which is a more general, language-agnostic standard. It provides an API that can be used from various programming languages (like C++, Python, PHP, Perl, etc.) to access a wide range of databases.
    *   These APIs are known as **`application programming interfaces (APIs)`**.

*   **`Database Programming Language Approach`**:
    *   Some DBMS vendors provide their own proprietary programming languages that are tightly integrated with SQL and the database itself. These languages often allow for the creation of stored procedures, functions, and triggers that execute directly within the database server.
    *   For example, **`ORACLE has PL/SQL`**, which is a procedural language extension to SQL. This language **`incorporates SQL and its data types as integral components`**, allowing for powerful server-side programming. Microsoft SQL Server has T-SQL (Transact-SQL), and other databases have similar proprietary extensions.

*   **`Scripting Languages`**:
    *   Many modern web applications and other types of programs are written using scripting languages, which often provide convenient ways to interact with databases.
    *   **`PHP`** (widely used for server-side web development) and **`Python`** (extremely popular for web development, data science, and general application development) are commonly **`used to write database programs`**. They typically use database connector libraries or APIs (which might themselves be built on top of ODBC/JDBC or native database drivers) to interact with databases. The slide mentions PHP for client-side scripting which might be a bit dated; its primary strength is server-side. Python is definitely a server-side powerhouse for database interaction.

Each of these approaches offers different trade-offs in terms of performance, portability, ease of development, and vendor lock-in.

---

## User-Friendly DBMS Interfaces (Slide 2-26)

Let's explore in more detail the characteristics of **`User-Friendly DBMS Interfaces`**. These are designed to make database interaction accessible and intuitive for end-users who are not necessarily programmers or database experts.

■ **`Menu-based (Web-based), popular for browsing on the web`**:
*   These interfaces guide users through a series of menus and submenus to find information or perform actions. This is a very common paradigm on websites, especially for e-commerce (browsing product categories) or information portals. The user clicks through options rather than formulating complex queries.

■ **`Forms-based, designed for naïve users used to filling in entries on a form`**:
*   These interfaces present users with on-screen forms that resemble paper forms. Users fill in data into clearly labeled fields. This is an excellent approach for structured data entry (e.g., entering a new customer record, submitting an order) and for simple queries where users can fill in criteria in specific fields. They are designed for users who are comfortable with form-filling but not with query languages.

■ **`Graphics-based`**:
*   These interfaces leverage visual elements to interact with the database. This can take several forms:
    *   **`Point and Click, Drag and Drop, etc.`**: Users might interact with graphical representations of data or operations.
    *   **`Specifying a query on a schema diagram`**: This is a powerful graphical approach where users can see a visual representation of the database schema (tables and their relationships). They might then click on tables, select attributes, and draw lines to specify joins, effectively building a query visually without writing SQL code. Tools like QBE (Query-By-Example) historically had graphical counterparts.

■ **`Natural language: requests in written English`**:
*   The ideal here is to allow users to make requests to the database using their everyday written language (e.g., "Show me all employees in the sales department hired last year"). While this is a very attractive goal, creating robust natural language interfaces that can accurately understand the full spectrum of complex human queries and translate them into precise database operations is a significant AI and NLP (Natural Language Processing) challenge. Progress is being made, but it's often limited to specific domains or simpler queries.

■ **`Combinations of the above`**:
*   In practice, many user-friendly interfaces don't stick rigidly to one type. They often combine elements from these different approaches to provide the best user experience.
    *   The slide highlights a common example: **`both menus and forms used extensively in Web database interfaces`**. A web application might use menus for navigation and then present forms for data entry or for specifying search criteria.

The primary aim of all these user-friendly interfaces is to lower the barrier to accessing and utilizing database information, making it available to a broader range of users.

---

## Other DBMS Interfaces (Slide 2-27)

Beyond the common categories of programmer interfaces and general user-friendly interfaces, there are **`Other DBMS Interfaces`** worth mentioning, catering to more specialized needs or newer interaction paradigms:

■ **`Natural language: free text as a query`**:
*   We touched on this before, but it's worth emphasizing that some systems, particularly in information retrieval and search engine technology, allow users to input queries as **`free text`**. The system then attempts to parse this text, identify keywords, and retrieve relevant information from the underlying database or document collection.

■ **`Speech : Input query and Output response`**:
*   This involves using spoken language for interaction. Users can **`Input query`** by speaking their request to the database system, and the system might provide an **`Output response`** by speaking the results back. This is becoming increasingly feasible with advances in speech recognition (voice-to-text) and speech synthesis (text-to-voice) technologies, seen in virtual assistants.

■ **`Web Browser with keyword search`**:
*   This is perhaps one of the most common ways many people interact with databases, often unknowingly. When you use a search engine or search within a large website, you are typically typing **`keywords`** into a search bar within a **`Web Browser`**. The backend system then translates these keywords into queries against a database to find and display relevant results.

■ **`Parametric interfaces, e.g., bank tellers using function keys.`**:
*   These are highly specialized and optimized interfaces designed for users who perform a limited set of predefined operations repeatedly and rapidly.
*   The classic example is **`bank tellers using function keys`**. Each function key on their terminal might be programmed to initiate a specific transaction (e.g., F1 for deposit, F2 for withdrawal, F3 for account balance). The interface is streamlined for these specific, high-frequency tasks, requiring minimal input beyond the core data for the transaction.

■ **`Interfaces for the DBA:`**:
*   Database Administrators (DBAs) have unique and powerful responsibilities, and they require specialized interfaces to manage and control the entire database system. These interfaces provide tools for:
    *   **`Creating user accounts, granting authorizations`** (i.e., permissions to access or modify data).
    *   **`Setting system parameters`** to configure and tune the performance of the DBMS.
    *   **`Changing schemas or access paths`** (e.g., adding or dropping tables, modifying columns, creating or removing indexes).
    *   Monitoring system health, managing backups, and performing other administrative tasks.

These varied interfaces cater to the diverse spectrum of interactions that occur with a database system, from casual searching to complex administration.

<div class="page-break"></div>

## Database System Utilities (Slide 2-28)

In addition to the core DBMS software (which handles data definition, manipulation, and querying), a comprehensive database system usually includes a suite of **`Database System Utilities`**. These are auxiliary programs or tools that perform various essential functions for managing, maintaining, and optimizing the database and its environment.

These utilities are designed **`To perform certain functions such as:`**

*   **`Loading data stored in files into a database.`**
    *   Often, data that needs to be brought into the database originates in external files, such as CSV (comma-separated values) files, text files, or spreadsheets. Loading utilities facilitate this bulk import process.
    *   These utilities often **`Includes data conversion tools`** because the format or structure of the data in the source files might need to be transformed to match the database schema requirements.

*   **`Backing up the database periodically on tape.`** (Or, more commonly today, to disk, network storage, or cloud storage).
    *   This is absolutely critical for disaster recovery. Backup utilities automate the process of creating copies of the database so that it can be restored in case of hardware failure, data corruption, or other catastrophic events.

*   **`Reorganizing database file structures.`**
    *   Over time, as data is inserted, updated, and deleted, the physical storage of database files can become fragmented or inefficiently organized. Reorganization utilities help to de-fragment files, reclaim unused space, and potentially re-cluster data to improve query performance.

*   **`Performance monitoring utilities.`**
    *   These tools are vital for DBAs. They help track the performance of the database system, identify bottlenecks (e.g., slow-running queries, high CPU or I/O usage), gather statistics about data distribution and query execution, and provide insights for tuning the database and applications.

*   **`Report generation utilities.`**
    *   While applications can certainly generate their own reports, some DBMSs or associated toolsets provide built-in utilities to create formatted reports directly from the database. These might offer features for summarizing data, creating charts, and distributing reports.

*   **`Other functions, such as sorting, user monitoring, data compression, etc.`**
    *   This is a catch-all for various other useful tools, which might include:
        *   **`Sorting`** utilities (though sorting is often handled directly by SQL's `ORDER BY` clause).
        *   **`User monitoring`** tools to track user activity for security auditing or resource usage analysis.
        *   **`Data compression`** utilities to reduce the amount of disk space required to store the database, which can also sometimes improve I/O performance.
        *   And many more, depending on the sophistication and features of the particular DBMS.

These utilities are indispensable for the day-to-day operation, administration, and maintenance of a healthy and efficient database system.

---

## Other Tools (Slide 2-29)

Beyond the core DBMS and its standard utilities, there are **`Other Tools`** that are often part of a comprehensive database environment, aiding in both the management of the database and the development of applications that use it.

■ **`Data dictionary / repository:`**
*   A data dictionary, or more broadly, a repository, is a crucial component. It's essentially a "database about the database" – it stores metadata.
*   It is **`Used to store schema descriptions`** (detailed information about tables, columns, data types, constraints, indexes, views, etc.).
*   But a comprehensive repository often stores much more than just the basic schema. It can include:
    *   **`Other information such as design decisions`** (e.g., rationale behind why certain schema choices were made).
    *   **`Application program descriptions`** (e.g., documentation about applications that access the database, and how they map to database schemas).
    *   **`User information`** (details about database users, their roles, and permissions).
    *   **`Usage standards`**, policies, and business rules related to the data.
    *   And so on. It serves as a central knowledge base about the database system.
*   An *active* data dictionary is one that is automatically maintained and used by the DBMS itself during its operations. A *passive* data dictionary is more for documentation and reference by humans.

■ **`Application Development Environments and CASE (computer-aided software engineering) tools:`**
*   These are sophisticated software tools designed to help developers design, build, test, and maintain database applications more efficiently and with higher quality.
*   They often provide a range of features, such as:
    *   Graphical interfaces for schema design (e.g., drawing ER diagrams that can then be translated into DDL).
    *   Code generation capabilities (e.g., generating boilerplate code for data access).
    *   Debugging tools specifically for database interactions.
    *   Version control integration.
    *   Project management features.
*   The slide mentions some examples:
    *   **`PowerBuilder (Sybase)`** – historically a popular rapid application development (RAD) tool for client-server database applications.
    *   **`JBuilder (Borland)`** – a well-known Integrated Development Environment (IDE) for Java, often used for database application development with JDBC.
    *   **`JDeveloper 10G (Oracle)`** – Oracle's own IDE, particularly strong for developing applications that interact with Oracle databases, supporting Java, PL/SQL, and other technologies.
*   These tools significantly enhance the productivity of application developers when working in a database-centric environment.

Both data dictionaries/repositories and CASE tools play vital roles in the effective management and utilization of database systems.

<div class="page-break"></div>

## Typical DBMS Component Modules

Let's now take a look at Figure 2.3, which illustrates the **Typical DBMS Component Modules** and their interactions. This diagram gives us a high-level view of the internal architecture of a DBMS. It's a bit complex, so let's break it down.

At the very top, we see different categories of **`Users`**:
*   **`DBA Staff`**: These are database administrators.
*   **`Casual Users`**: Those who interact with the database occasionally, perhaps through ad-hoc queries.
*   **`Application Programmers`**: Developers who write programs that access the database.
*   **`Parametric Users`**: Users who primarily run pre-defined transactions, like bank tellers.

These users interact with the DBMS through various inputs:
*   DBA Staff submit **`DDL Statements`** (to define schemas) and **`Privileged Commands`** (for control and management).
*   Casual Users submit **`Interactive Queries`**.
*   Application Programmers create **`Application Programs`** that contain embedded DML or API calls.
*   Parametric Users typically execute **`Compiled Transactions`** that were prepared by application programmers.

Now, let's trace how these inputs are processed. This central part of the diagram can be broadly divided into "Query Processor Components" and "Storage Manager Components."

Under **`Query Processor Components`**:
*   **`DDL Statements`** go to the **`DDL Compiler`**, which processes these definitions and updates the **`System Catalog/Data Dictionary`** (which stores all the metadata).
*   **`Interactive Queries`** from casual users go to the **`Query Compiler`**. This compiler parses the query, checks its syntax and semantics, and then passes it to the **`Query Optimizer`**.
*   The **`Query Optimizer`** is a critical component. It analyzes the query and consults the System Catalog (for information about data structures, indexes, statistics, etc.) to generate an efficient *execution plan* – the best strategy for retrieving the requested data.
*   **`Application Programs`** are first processed by a **`Precompiler`** (if they contain embedded DML). The precompiler separates DML statements, which are sent to the **`DML Compiler`**, from the host language code, which goes to a **`Host Language Compiler`**.
*   The **`DML Compiler`** also interacts with the Query Optimizer to get efficient execution plans for the DML statements.
*   The output of the DML Compiler and the Host Language Compiler are combined to produce **`Compiled Transactions`** or executable code.

These compiled transactions or optimized query execution plans are then handled by the **`Runtime Database Processor`**. This component oversees the execution of these plans. It interacts with several other modules:
*   It accesses the **`System Catalog/Data Dictionary`** for schema information.
*   It interacts with the **`Concurrency Control`** and **`Backup/Recovery Subsystems`**. These are crucial for ensuring data integrity, managing simultaneous access by multiple users, and recovering from failures.
*   The Runtime Database Processor ultimately sends requests to the **`Stored Data Manager`**.

Under **`Storage Manager Components`**:
*   The **`Stored Data Manager`** (also sometimes called the buffer manager or file manager) is responsible for handling all interactions with the physical database. It translates the logical requests from the runtime processor into physical operations on the stored data.
*   It manages the transfer of data between the disk (**`Stored Database`**) and main memory buffers.
*   The **`Concurrency Control/Backup/Recovery Subsystems`** work closely with the Stored Data Manager to ensure transaction atomicity, consistency, isolation, and durability (the ACID properties), and to handle logging for recovery.
*   Finally, the Stored Data Manager performs the actual **`Input/Output from Database`** on disk.

The dashed lines in the diagram often indicate control flow or access to metadata (like the System Catalog), while solid lines typically indicate data flow or processing steps.

So, as you can see, a DBMS is a complex system with many interacting modules, all working together to manage data efficiently and reliably. This diagram, titled "Figure 2.3 Component modules of a DBMS and their interactions," provides a good conceptual map.

<div class="page-break"></div>

## Centralized and Client-Server DBMS Architectures

Now let's shift our focus to how Database Management Systems are architecturally deployed. We'll start with **`Centralized DBMS Architectures`**.

■ A **`Centralized DBMS`**:
*   In this architecture, everything is essentially combined into a **`single system`**. This includes the DBMS software itself, the hardware it runs on, the application programs that use the database, and even the user interface processing software.
*   Think of a traditional mainframe environment. All the processing power, data storage, and software reside on one central computer.
*   It's important to note that users can still connect to this centralized system through **`remote terminals`**. However, these terminals are typically "dumb" terminals or thin clients; all the actual data processing, query execution, and application logic happens at the **`centralized site`**. The terminal is just for input and output display.

This was a very common architecture in the past, and it still has its uses where tight control and centralized management are paramount.

---

### A Physical Centralized Architecture

Figure 2.4 provides a visual representation of **`A Physical Centralized Architecture`**. Let's walk through it.

On the left, we see several **`Terminals`**, each with a **`Display Monitor`**. These could be numerous. These terminals are connected via a **`Network`** to the central system.

The main box represents the **`Centralized System`**. It's internally divided into:

*   **`Software`**: This layer includes:
    *   **`Application Programs`**: The actual business applications.
    *   **`Terminal Display Control`**: Software to manage the interaction with the remote terminals.
    *   **`Text Editors`**, **`Compilers`**: Development tools.
    *   The **`DBMS`** software itself.
    *   And other software components (`...`).

*   Below the software layer is the **`Operating System`** (OS), which manages the hardware resources and provides services to the software layer.

*   At the bottom is the **`Hardware/Firmware`** layer:
    *   The **`CPU`** (Central Processing Unit) is the brain of the system.
    *   A **`System Bus`** connects various hardware components.
    *   Attached to the bus are **`Controllers`** for different devices.
    *   We have **`Memory`** (RAM).
    *   **`Disk`** storage, where the database itself resides.
    *   **`I/O Devices`** like printers, tape drives for backups, etc.
    *   And potentially other hardware components (`...`).

This diagram clearly shows all processing and data storage consolidated in one physical location. The terminals are merely access points. This is the classic mainframe model.

<div class="page-break"></div>

## Basic 2-tier Client-Server Architectures

Now let's move to a more distributed model: **`Basic 2-tier Client-Server Architectures`**. This became very popular with the rise of personal computers and local area networks.

■ The core idea is to have **`Specialized Servers with Specialized functions`**. Instead of one monolithic central system, functionality is distributed across different servers.
*   For example, you might have a **`Print server`** dedicated to managing print jobs.
*   A **`File server`** dedicated to storing shared files.
*   A **`DBMS server`** (or database server) dedicated to running the DBMS and managing the database. This is our primary interest.
*   A **`Web server`** for hosting web pages and web applications.
*   An **`Email server`** for handling electronic mail.

The diagram illustrates this:
*   We have multiple **`Clients`** (Client, Client, Client, ...). These are typically user workstations or PCs.
*   The clients are connected via a **`Network`** (like a LAN or WAN).
*   On the other side of the network are the specialized servers: **`Print Server`**, **`File Server`**, **`DBMS Server`**, and so on (`...`).

■ **`Clients can access the specialized servers as needed`**. So, a client machine might send a document to the print server, retrieve a file from the file server, and query the DBMS server for data, all through the network. This division of labor is a key characteristic of client-server architectures.

---

### Clients

In a client-server architecture, what exactly are the **`Clients`**?

■ Clients **`provide appropriate interfaces`** through a client software module. This software allows the client machine to access and utilize the resources provided by the various servers.
*   For example, to interact with a DBMS server, the client machine would need client-side database connectivity software (like an ODBC driver or JDBC driver and perhaps a query tool).

■ Clients can take various forms:
*   They may be **`diskless machines`** that rely entirely on servers for storage and applications.
*   Or, more commonly, they are **`PCs or Workstations with disks`**. These machines would have only the necessary **`client software installed`**, while the server software (like the DBMS itself) resides on the server machine.

■ Clients are **`connected to the servers via some form of a network`**.
*   This network could be a **`LAN (local area network)`** within an office, a **`wireless network`**, or even a wide area network (WAN) like the internet.

The client is essentially the service *requester* in this model.

---

### DBMS Server

Now, let's focus on the **`DBMS Server`** in a client-server architecture.

■ The DBMS server **`provides database query and transaction services to the clients`**. It hosts the actual database and the DBMS software that manages it. Clients send requests (queries or updates) to the DBMS server, and the server processes these requests and sends results back to the clients.

■ **`Relational DBMS servers`** are often specifically called **`SQL servers`** (because they process SQL queries), **`query servers`**, or **`transaction servers`** (because they manage database transactions).

■ **`Applications running on clients`** utilize an **`Application Program Interface (API)`** to access server databases. This API provides a standard way for client applications to communicate with the DBMS server.
*   Common standard interfaces include:
    *   **`ODBC (Open Database Connectivity)`**: This is a widely adopted standard API for accessing databases. It allows an application to connect to various different DBMSs using a common set of functions, provided the appropriate ODBC driver for each DBMS is available.
    *   **`JDBC (Java Database Connectivity)`**: This is the standard API for Java programming access to databases. Like ODBC, it provides a consistent way for Java applications to connect to and interact with different database servers.

The DBMS server is the workhorse that handles all the core database management tasks.

---

### Two Tier Client-Server Architecture

Let's consider some more details of the **`Two-Tier Client-Server Architecture`**, specifically in the context of database access.

■ For this to work, the **`client and server must install appropriate client module and server module software`**.
*   For example, if using ODBC or JDBC, the client machine needs the relevant ODBC/JDBC drivers and client-side libraries. The server machine needs the DBMS server software and the corresponding server-side components to listen for and process client requests via ODBC/JDBC.

■ A **`client program may connect to several DBMSs`**. These different DBMSs that a client might connect to are sometimes collectively referred to as **`data sources`**.
*   For example, an application on a client might need to pull customer data from an Oracle database and product data from an SQL Server database. ODBC and JDBC are designed to facilitate this kind of heterogeneous access.

■ In general, **`data sources can be files or other non-DBMS software that manages data`**. While our focus is on DBMSs, the concept of a data source is broader. An ODBC driver might exist, for example, to access data in a simple CSV file or an Excel spreadsheet as if it were a database table.

■ The textbook mentions, "See Chapter 10 for details on Database Programming." Chapter 10 will delve much deeper into how application programs are actually written to interact with databases using these client-server mechanisms and APIs like JDBC and ODBC.

This two-tier model separates the presentation logic (on the client) from the data management logic (on the server).

<div class="page-break"></div>

### Three Tier Client-Server Architecture

While the two-tier architecture is useful, many modern applications, especially web applications, use a **`Three-Tier Client-Server Architecture`**. This introduces an additional middle layer.

■ This architecture is very **`common for Web applications`**.

■ The key addition is an **`Intermediate Layer`**, often called an **`Application Server`** or a **`Web Server`** (though a web server might primarily serve static content and pass dynamic requests to an application server).
*   This middle tier typically **`stores the web connectivity software`** (e.g., software to handle HTTP requests and responses) and, importantly, the **`business logic`** part of the application. The business logic is the set of rules and procedures that govern how data is created, manipulated, and accessed. For example, how an order is processed, or how a discount is calculated. This logic is used to access the corresponding data from the database server.
*   The application server **`acts like a conduit`** for sending partially processed data between the database server and the client. For example, it might retrieve raw data from the database, apply some business rules or formatting, and then send the refined information to the client. It can also receive input from the client, validate it, apply business rules, and then interact with the database server.

■ The **`Three-tier Architecture Can Enhance Security`**:
*   One significant advantage is that the **`database server is only accessible via the middle tier`**.
*   **`Clients cannot directly access the database server`**. All requests must go through the application server, which can enforce security policies and business rules. This creates an additional layer of protection for the database.
*   **`Clients`** in this model typically contain the **`user interfaces`** (e.g., rendered by a web browser) and **`Web browsers`** themselves.
*   The **`client`** is often a **`PC or a mobile device connected to the Web`**, interacting with the application server via standard web protocols like HTTP/HTTPS.

This three-tier (or n-tier, with more middle layers) architecture provides better scalability, flexibility, and often improved security compared to the two-tier model.

---

### Three-tier client-server architecture

Figure 2.7 illustrates this **`three-tier client-server architecture`** with a couple of common ways to conceptualize the layers, labeled (a) and (b).

**In view (a), labeled "Component View":**
*   At the top, we have the **`Client`**. This tier is responsible for the **`GUI (Graphical User Interface)`** or **`Web Interface`** – what the user sees and interacts with.
*   In the middle is the **`Application Server or Web Server`**. This tier hosts the **`Application Programs`** and **`Web Pages`** (dynamic content generation). This is where the business logic resides.
*   At the bottom is the **`Database Server`**. This tier runs the **`Database Management System (DBMS)`** and manages the actual data.

The client communicates with the application server, and the application server communicates with the database server. There's no direct communication between the client and the database server.

**In view (b), labeled "Layered View," which is a more abstract way to think about the roles:**
*   The top tier (Client) is called the **`Presentation Layer`**. Its job is to present information to the user and collect input from the user.
*   The middle tier (Application Server/Web Server) is called the **`Business Logic Layer`** (or Application Layer). It implements the core functionality and business rules of the application.
*   The bottom tier (Database Server) is called the **`Database Services Layer`** (or Data Access Layer). Its job is to provide persistent storage and access to data.

Again, the layers interact sequentially. The presentation layer talks to the business logic layer, which in turn talks to the database services layer.

This separation of concerns into three distinct tiers is a powerful architectural pattern.

<div class="page-break"></div>

## Classification of DBMSS

Database Management Systems can be classified in several ways based on different criteria.

■ First, a very common classification is **`Based on the data model used`**:
*   **`Legacy`** data models: These are older models that are still in use in some existing systems but are generally not chosen for new development.
    *   Examples include the **`Network`** model and the **`Hierarchical`** model.
*   **`Currently Used`** data models: These are the dominant models in modern database systems.
    *   The **`Relational`** model is by far the most widely used.
    *   **`Object-oriented`** data models aim to integrate database capabilities with object-oriented programming concepts.
    *   **`Object-relational`** data models are a hybrid, extending the relational model with object-oriented features.
*   **`Recent Technologies`**: This category includes newer approaches, often falling under the umbrella of NoSQL (Not Only SQL) or NewSQL.
    *   **`Key-value storage systems`** (e.g., Redis, DynamoDB).
    *   **`NOSQL systems`** in general, which can be:
        *   **`Document based`** (e.g., MongoDB, Couchbase), storing data in flexible document formats like JSON or BSON.
        *   **`Column-based`** (or wide-column stores, e.g., Cassandra, HBase), which store data in columns rather than rows, good for analytical workloads.
        *   **`Graph-based`** (e.g., Neo4j, Amazon Neptune), which are optimized for data with complex relationships, like social networks or recommendation engines.
        *   And also **`key-value based`** as mentioned above.
    *   **`Native XML DBMSs`**, which are designed specifically for storing and querying data in XML format.

■ **`Other classifications`** are also important:
*   **`Single-user`** vs. **`multi-user`**:
    *   A **`single-user`** DBMS is designed to support only one user at a time. These are typically used with personal computers for individual applications (e.g., older versions of Microsoft Access might run in a single-user mode by default on a local file).
    *   A **`multi-user`** DBMS, which includes *most* DBMSs (like Oracle, SQL Server, MySQL, PostgreSQL), is designed to support concurrent access by many users and applications.
*   **`Centralized`** vs. **`distributed`**:
    *   A **`centralized`** DBMS uses a single computer (or a tightly coupled cluster perceived as a single computer) with one database.
    *   A **`distributed`** DBMS involves multiple computers and potentially multiple databases, which may be geographically dispersed but are managed as a single logical system. We'll look more into variations of distributed DBMSs next.

Understanding these classifications helps in choosing the right type of DBMS for a particular application or need.

---

### Variations of Distributed DBMSs (DDBMSs)

Let's briefly look at some variations when we talk about **`Distributed Database Management Systems (DDBMSs)`**:

■ **`Homogeneous DDBMS`**:
*   In a homogeneous DDBMS, all sites (nodes in the distributed system) use the *same* DBMS software and the same data model. This is the simplest form to manage because the software is consistent across the network.

■ **`Heterogeneous DDBMS`**:
*   In a heterogeneous DDBMS, different sites may run *different* DBMS software (e.g., one site runs Oracle, another runs SQL Server) and potentially use different data models. This is much more complex to manage, requiring sophisticated translation and coordination mechanisms.

■ **`Federated or Multidatabase Systems`**:
*   This is a type of heterogeneous DDBMS where the participating databases are often pre-existing and autonomous.
*   The **`Participating Databases are loosely coupled`** and maintain a **`high degree of autonomy`**. Each local database continues to operate independently, but the federated system provides a way to access and integrate data from these autonomous sources as if they were part of a single logical database.

■ An interesting evolution in terminology: **`Distributed Database Systems have now come to be known as client-server based database systems because`**:
*   Many systems that are termed "distributed" today don't necessarily support a *totally* distributed environment where data and processing can be arbitrarily fragmented and located anywhere with full transparency.
*   Instead, they often operate as a **`set of database servers supporting a set of clients`**. While the servers themselves might be distributed and perhaps replicate or partition data among themselves, the interaction model often still follows a client-server pattern. The "distribution" is more about how the server-side is implemented and managed.

True, fully transparent distributed database systems are complex, but the principles of distributing data and processing are widely applied in modern architectures.

---

### Cost considerations for DBMSS

When selecting or deploying a DBMS, **`cost`** is, of course, a significant factor. The costs can vary dramatically.

■ **`Cost Range`**: The cost can range from **`free open-source systems`** to highly sophisticated commercial configurations **`costing millions of dollars`** (including software licenses, hardware, support, and personnel).

■ **`Examples of free relational DBMSs`**: There are many excellent open-source options available at no licensing cost.
*   **`MySQL`** (now owned by Oracle, but with a free community edition)
*   **`PostgreSQL`** (a very powerful, feature-rich open-source RDBMS)
*   And **`others`** like SQLite (for embedded databases), MariaDB (a fork of MySQL).

■ **`Commercial DBMS offer additional specialized modules`**: Commercial vendors often provide their core DBMS and then offer add-on modules for specialized functionalities, which usually come at an additional cost.
*   For example:
    *   A **`time-series module`** for handling time-stamped data efficiently.
    *   A **`spatial data module`** for managing geographic information (GIS data).
    *   A **`document module`** for handling semi-structured documents.
    *   An **`XML module`** for native XML data processing.
*   These modules offer **`additional specialized functionality when purchased separately`**.
*   Sometimes these add-on modules are called **`cartridges`** (e.g., in Oracle terminology) or **`blades`**.

■ **`Different licensing options`**: Commercial DBMS software is typically licensed in various ways:
*   **`Site license`**: Allows an organization to use the software on many machines at a specific site.
*   **`Maximum number of concurrent users`** (or **`seat license`**): The cost is based on how many users can access the system simultaneously or how many named users are licensed.
*   **`Per-processor`** or **`per-core`** license: Cost is based on the processing power of the server hardware.
*   **`Single user`** license: For individual use.
*   And various other models, including subscription-based licensing for cloud DBMS offerings.

It's crucial to consider the total cost of ownership, not just the initial software license fees.

---

### Other Considerations

Beyond data models and cost, there are other important considerations when evaluating or working with database systems:

■ **`Type of access paths within database system`**:
*   The internal mechanisms used by the DBMS to locate and retrieve data can significantly impact performance.
*   For example, some systems heavily rely on **`inverted indexing`**.
    *   The slide mentions **`ADABAS`** (a historically significant DBMS) as one such system. Inverted indexes are like the index at the back of a book; for each keyword (or attribute value), they list all the records/documents that contain it.
    *   **`Fully indexed databases`** that provide access by *any* keyword are fundamental to how **`search engines`** work. They allow for very fast retrieval based on keyword searches.

■ **`General Purpose vs. Special Purpose`** DBMS:
*   Most of the DBMSs we discuss (like Oracle, SQL Server, PostgreSQL) are **`general-purpose`**. They are designed to be flexible and support a wide variety of applications.
*   However, some database systems are **`special-purpose`**, designed and optimized for a very specific type of application.
    *   For example, **`Airline Reservation systems`** are classic examples. They handle a very high volume of specific types of transactions (booking, checking availability, etc.) and have stringent performance and reliability requirements.
    *   Many other reservation systems (for hotels, cars, etc.) also fall into this category.
    *   These are often **`special-purpose OLTP (Online Transaction Processing) Systems`**. They are highly tuned for handling a large number of short, fast, concurrent transactions.

The choice between general-purpose and special-purpose depends on the unique requirements of the application.

<div class="page-break"></div>

## History of Data Models (Additional Material)

The textbook provides additional material on the history of data models, which is very insightful for understanding how database technology has evolved to its current state. We won't cover this historical material in exhaustive detail now, but it's good to be aware of the main milestones.

The key data models in this historical progression include:

■ **`Network Model`**
■ **`Hierarchical Model`**
(These two are considered legacy models, as we mentioned earlier. They were important predecessors to the relational model.)

■ **`Relational Model`**
(This was a revolutionary development and remains dominant today.)

■ **`Object-oriented Data Models`**
(These emerged with the rise of object-oriented programming.)

■ **`Object-Relational Models`**
(These attempt to combine the strengths of the relational and object-oriented approaches.)

Understanding this history provides context for the features and design choices in modern DBMSs.

---

### Relational Model:

Let's briefly touch upon the **`Relational Model`**, as it's so central to modern database systems. (The slide number indicates this is out of sequence with a full historical overview, perhaps focusing on key models.)

*   The Relational Model was **`Proposed in 1970 by E.F. Codd`** (who worked for IBM at the time). This was a landmark paper that laid the theoretical foundation. The **`first commercial system`** based on this model appeared around **`1981-82`**.
*   It is **`now in several commercial products`**, indeed, it powers most of them. Examples include IBM's **`DB2`**, **`ORACLE`**, Microsoft **`MS SQL Server`**, **`SYBASE`** (now SAP ASE), and **`INFORMIX`** (now IBM Informix).
*   There are also **`several free open source implementations`** that are very popular and widely used, such as **`MySQL`** and **`PostgreSQL`**.
*   The relational model is **`currently most dominant for developing database applications`**. Its simplicity, theoretical soundness, and the power of SQL have made it incredibly successful.
*   There are **`SQL relational standards`** that have evolved over time to ensure interoperability and define common features. These include **`SQL-89`** (also known as SQL1), **`SQL-92`** (SQL2), **`SQL-99`** (SQL3), and subsequent revisions.
*   The textbook dedicates **`Chapters 5 through 11`** to describing this model in detail, which underscores its importance. We will be spending a significant amount of time on these chapters.

The relational model truly transformed the database landscape.

---

### Object-oriented Data Models:

Following the relational model, and with the rise of object-oriented programming languages, **`Object-oriented Data Models`** emerged.

*   **`Several models have been proposed`** for implementing object-oriented concepts directly within a database system. The goal was to reduce the "impedance mismatch" between object-oriented application code and relational databases.
*   One set of these comprises models of **`persistent O-O Programming Languages`**. The idea is to make objects created in a language like C++ or Smalltalk persistent – meaning they can be stored in a database and survive beyond the execution of the program.
    *   Examples include systems like **`OBJECTSTORE`** or **`VERSANT`** (for C++), and **`GEMSTONE`** (for Smalltalk).
*   Additionally, there were dedicated **`Object-Oriented Database Systems (OODBS)`** like **`O2`**, **`ORION`** (developed at MCC, later became ITASCA), and **`IRIS`** (developed at H.P., which was used in an early open-source OODB called OpenOODB).
*   To promote standardization, the Object Database Management Group (ODMG) developed the **`Object Database Standard`**. There were several versions, including **`ODMG-93`**, **`ODMG-version 2.0`**, and **`ODMG-version 3.0`**. This standard defined a common object model, object definition language (ODL), object query language (OQL), and language bindings for C++ and Smalltalk.
*   The textbook mentions that **`Chapter 12 describes this model`**.

While OODBs didn't displace relational databases as the dominant technology, many of their concepts influenced the development of object-relational models and are still relevant in certain niche applications.

---

### Object-Relational Models:

Given the strengths of both the relational model (simplicity, strong theoretical foundation, SQL) and object-oriented models (complex data types, inheritance, methods), there was a move towards **`Object-Relational Models (ORDBMS)`**.

*   The **`trend to mix object models with relational`** concepts was notably started with **`Informix Universal Server`**. Informix was a pioneer in adding object-like features (such as user-defined data types and functions) to its relational core.
*   Subsequently, other major **`Relational systems incorporated concepts from object databases`**, leading to what we call object-relational databases. They extended the relational model to support richer data types, inheritance, and other object features.
*   This approach is **`exemplified in the versions of Oracle, DB2, and SQL Server and other DBMSs`**. Most major commercial relational databases today have significant object-relational capabilities. For instance, you can define custom data types, methods, and table inheritance in many of these systems.
*   The **`current trend by Relational DBMS vendors is to extend relational DBMSs with capability to process XML, Text and other data types`**. This continues the theme of enhancing the relational model to handle more diverse and complex types of data beyond simple atomic values.
*   Interestingly, the textbook notes that **`The term "Object-relational" is receding in the marketplace`**. While the *features* are widely adopted and very much present in modern "relational" databases, the explicit label "object-relational" as a distinct category seems to be used less frequently. The capabilities have largely been absorbed into what is now considered a modern, extended relational DBMS.

This fusion of ideas has led to more powerful and flexible database systems.

<div class="page-break"></div>

## Chapter Summary

Alright, let's summarize the key topics we've covered in this chapter on Database System Concepts and Architecture.

*   We started with **`Data Models and Their Categories`**, understanding what a data model is and looking at conceptual, physical, implementation, and self-describing models.
*   We then distinguished between **`Schemas, Instances, and States`**, clarifying that the schema is the blueprint and the state/instance is the actual data content.
*   We discussed the **`Three-Schema Architecture`** (external, conceptual, internal levels) as a foundational concept for understanding database organization.
*   This led to the important property of **`Data Independence`** (both logical and physical), which this architecture facilitates.
*   We explored **`DBMS Languages and Interfaces`**, including DDL, DML (high-level/non-procedural vs. low-level/procedural), and various user and programmer interfaces.
*   We looked at **`Database System Utilities and Tools`**, such as those for loading, backup, performance monitoring, and data dictionaries.
*   We briefly considered the overall **`Database System Environment`** by looking at a diagram of typical DBMS component modules.
*   We examined different deployment models: **`Centralized and Client-Server Architectures`** (2-tier and 3-tier).
*   We discussed the **`Classification of DBMSs`** based on data model, number of users, and distribution.
*   And finally, we touched upon the **`History of Data Models`**, highlighting the evolution from legacy models to relational, object-oriented, and object-relational systems.

This chapter has laid a lot of important groundwork. These concepts will recur throughout our study of database systems. Any questions?