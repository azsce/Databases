---
title: "1. chapter 1"
---

# Databases and Database Users


# FUNDAMENTALS OF DATABASE SYSTEMS, 7th Edition, by Elmasri and Navathe

# CHAPTER 1: Databases and Database Users

Our first chapter, Chapter 1, is titled "Databases and Database Users." This chapter will serve as our introduction to the entire field. We'll define what databases are, explore who uses them, and understand why they are so critical in today's world. Think of this as laying the groundwork... the essential vocabulary and concepts upon which everything else will be built.

<div class="page-break"></div>

### OUTLINE

So, what exactly will we cover in this introductory chapter? Let's look at the outline.

First, we'll discuss **Types of Databases and Database Applications**. You'll see that databases come in many flavors and serve a vast array of purposes.
Then, we'll establish some **Basic Definitions** – key terms you'll hear repeatedly.
We'll move on to **Typical DBMS Functionality**. DBMS stands for Database Management System, and we'll explore what these systems actually *do*.
To make things more concrete, we'll look at an **Example of a Database**, specifically a UNIVERSITY database. This will help illustrate the concepts.
A very important section will be on the **Main Characteristics of the Database Approach**. Why choose this approach over older methods?
We'll then categorize the **Types of Database Users**. Who are the people interacting with these systems?
Following that, we'll summarize the **Advantages of Using the Database Approach**.
We'll also take a brief look at the **Historical Development of Database Technology** to understand how we got here.
We'll touch upon **Extending Database Capabilities** – how databases are evolving.
And finally, we'll consider **When Not to Use Databases**. Yes, there are situations where a full-fledged database system might be overkill.

A comprehensive overview, indeed. Let's proceed.

<div class="page-break"></div>

### What is data, database, DBMS

Let's start with the absolute fundamentals. What are we even talking about?

First, **`Data`**. Data simply refers to known facts that can be recorded and have an implicit meaning. Think of it as raw material – names, numbers, dates, images. For example, "John Smith," the number "35," or a picture of a product.

Next, a **`Database`**. A database is a highly organized, interrelated, and structured set of data about a particular enterprise. The key words here are "organized," "interrelated," and "structured." It's not just a random collection of data; it’s data that’s meaningfully connected and arranged. And very importantly, a database is typically controlled by a **database management system**, or **DBMS**.

So, what is a **DBMS**? A DBMS is a set of programs designed to access the data. It provides an environment that is both *convenient* and *efficient* to use. It's the software that sits between the users (or applications) and the actual data, managing all interactions.

Database systems, then, are used to manage collections of data that are typically:
*   **`Highly valuable`** – think of financial records, customer information, or scientific research.
*   **`Relatively large*`* – we're often dealing with vast amounts of information.
*   And **accessed by multiple users and applications, often at the same time**. This concurrency aspect is crucial.

A modern database system, therefore, is a complex software system. Its primary task is to manage a large, complex collection of data. And make no mistake, databases touch all aspects of our lives, from online shopping to social media, to how organizations run.

<div class="page-break"></div>

### Types of Databases and Database Applications

Now that we have a basic understanding, let's consider the different types of databases and their applications.

We can broadly categorize them. First, **Traditional applications**:
*   These primarily involve **Numeric and textual databases**. Think of your classic accounting systems, inventory management, or library catalogs. This has been the bread and butter of database applications for decades.

However, the landscape has evolved significantly. We now have many **More recent applications**:
*   **Multimedia databases**, which store and manage images, audio, and video.
*   **Geographic Information Systems**, or **GIS**, which handle spatial data like maps and locations.
*   **Biological and genome databases**, crucial for scientific research, storing complex biological sequences.
*   **Data warehouses**, which are specialized databases designed for analysis and reporting, often consolidating data from multiple sources.
*   **Mobile databases**, which reside on smartphones and other mobile devices.
*   And **Real-time and active databases**, which need to respond to events and process data with very low latency, and can even trigger actions automatically.

For the first part of this book, our focus will be primarily on **traditional applications**, as they provide the foundational concepts. However, it's important to be aware that a number of these recent applications, and the technologies behind them, will be described later in the book, for instance, in Chapters 24 through 29. The field is constantly expanding.

<div class="page-break"></div>

### Recent Developments (1)

Let's touch upon some recent developments that are profoundly impacting the database world.

Firstly, **Social Networks**. Platforms like Facebook, Twitter, and LinkedIn have started capturing an immense amount of information. This includes data about people, their connections, and their communications – things like posts, tweets, photos, and videos. All of this diverse information constitutes data that needs to be managed.

Secondly, **Search Engines**. Giants like Google, Bing, and Yahoo collect and maintain their own massive repositories of web pages. They do this for searching purposes, allowing us to find information across the vast expanse of the internet. This, too, is a monumental data management challenge.

These examples highlight the sheer scale and variety of data being generated and managed today.

<div class="page-break"></div>

### Recent Developments (2)

Continuing with recent developments, new technologies are emerging, particularly from what we call **non-SQL** or **non-database software vendors**. These are often designed to manage the vast amounts of data generated on the web. Two key trends here are:

*   **Big data storage systems**. These systems often involve large clusters of distributed computers. We'll delve into this in more detail in Chapter 25. The idea is to distribute the data and the processing load across many machines to handle the scale.
*   **NOSQL systems**. NOSQL stands for "Non-SQL" or, perhaps more accurately, "Not Only SQL." These are alternative database systems that often relax some of the strict consistency rules of traditional relational databases to achieve better scalability and flexibility for certain types of web-scale applications. Chapter 24 will cover these.

Furthermore, a large amount of data now resides on the **"cloud."** What this means is that the data is stored in huge data centers, often using thousands of machines, managed by cloud providers. This has significant implications for how data is accessed, secured, and managed.

<div class="page-break"></div>

### What is "big data"?

We've mentioned "big data." But what exactly *is* it?

Gartner, a well-known research firm, provided a widely cited definition in 2012: **"Big data are high-volume, high-velocity, and/or high-variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization."**

Let's break that down.
*   **High-volume**: We're talking about sheer size – terabytes, petabytes, and beyond.
*   **High-velocity**: Data is arriving at an incredibly fast rate, needing to be processed quickly. Think of sensor data or social media feeds.
*   **High-variety**: Data comes in many different formats – structured, unstructured (like text or video), and semi-structured.

This definition often refers to the "Three Vs." But are there other "Vs"? Indeed. Some add:
*   **Veracity**: This refers to the trustworthiness or quality of the data. With so much data, ensuring its accuracy is a challenge.
*   **Value**: Ultimately, the goal is to extract value from this data. Will it lead to the discovery of a critical causal effect or provide a competitive advantage?

So, the **bottom line** is: Any data that exceeds our current capability of processing can be regarded as "big." It's a relative term. What was "big" yesterday might be manageable today with new tools.

Also, it's worth noting that **complicated or intelligent analysis of data may make even a seemingly small dataset "appear" to be "big"** in terms of the processing power and sophisticated techniques required to extract insights from it.

<div class="page-break"></div>

### Why is "big data" a "big deal"?

Why all the fuss about "big data"? Why is it such a "big deal"? Let's look at some examples across different sectors.

In **Government**, big data is used for everything from policy-making to security.

In the **Private Sector**, the impact is enormous:
*   Consider **Walmart**. They handle more than 1 million customer transactions *every hour*. This data is imported into databases estimated to contain more than 2.5 *petabytes* of data. That's an astonishing amount.
*   **Facebook** handles 40 *billion* photos from its user base. Imagine the storage and retrieval challenges.
*   The **Falcon Credit Card Fraud Detection System** protects 2.1 *billion* active accounts worldwide, analyzing transaction patterns in real-time to detect fraud.

And in **Science**:
*   The **Large Synoptic Survey Telescope**, when operational, is projected to generate 140 *Terabytes* of data every 5 days. This will revolutionize astronomy.
*   **Biomedical computation**, such as decoding the human genome and developing personalized medicine, relies heavily on analyzing massive datasets.

These examples illustrate that big data isn't just a buzzword; it's a reality that is transforming industries and driving innovation.

<div class="page-break"></div>

### Lifecycle of Data: 4 "A"s

This diagram illustrates a conceptual lifecycle of data, often characterized by four "A"s. Let's walk through it.

We start with **Acquisition**. This is where data is initially gathered. It often comes in as **Scattered Data** from various sources. Think of sensor readings, user inputs, or external feeds.

From Acquisition, we move to **Aggregation**. Here, the scattered data is brought together, perhaps cleaned, and combined. The output of this stage is often **Integrated Data**, which is more coherent and consolidated.

Next comes **Analysis**. The integrated data is then analyzed to extract meaningful insights and patterns. This process generates **Knowledge**.

Finally, this Knowledge is put to use in an **Application**. The application might be a decision-support system, a predictive model, or a new service. The use of this application, in turn, often generates more data, perhaps in the form of **Log data** or new transactional data, which then feeds back into the Acquisition phase, creating a continuous cycle.

So, it's a flow: Scattered Data is Acquired, Aggregated into Integrated Data, Analyzed to produce Knowledge, which is then Applied, potentially generating new Log Data to start the cycle anew. This cyclical nature is key to continuous improvement and learning from data.

<div class="page-break"></div>

### Computational View of Big Data

This slide presents a computational view of processing Big Data, showing a pipeline of stages.

At the very bottom, we have raw **Data** entering the system.
The first step is **Storage**. The data needs to be stored efficiently and reliably.
Following storage, we often have **Formatting and Cleaning**. Raw data is rarely perfect; it might have errors, inconsistencies, or be in different formats. This stage prepares the data for further processing.
Once cleaned, we move to **Data Understanding**. This involves exploring the data, understanding its structure, its content, and its potential.
From Data Understanding, we can go in a couple of directions.
One path leads to **Data Access**, which allows users or applications to retrieve the data they need. Data Access, in turn, often feeds into **Data Visualization**, where data is presented graphically to help humans understand patterns and insights.
The other path from Data Understanding is **Data Integration**. This is where data from different sources might be combined.
Integrated data then typically goes into **Data Analysis**, where advanced algorithms and statistical techniques are applied to discover patterns, build models, or make predictions.
And, like Data Access, the results of Data Analysis also often feed into **Data Visualization** to communicate the findings.

So, it's a flow: from raw Data, through Storage, Formatting/Cleaning, Understanding, then branching to Access and Integration, both leading to Analysis, and ultimately, Visualization. This represents a common workflow for making sense of large datasets.

<div class="page-break"></div>

### Big Data & Related Disciplines

This diagram expands on the previous one, showing the relationship between the stages of Big Data processing and various academic and technical disciplines. It's a bit busy, so let's break it down.

Imagine the same pipeline as before, starting from the bottom:
*   **Data** is the input.
*   **Storage**: This stage is closely related to **Information Theory**, which deals with the quantification, storage, and communication of information.
*   **Formatting, Cleaning**: This often involves techniques from **Signal Processing**, especially when dealing with sensor data or other types of signals.
*   **Data Understanding**: This is a rich area drawing from multiple disciplines:
    *   **Natural Language Processing (NLP)** for understanding text.
    *   **Computer Vision** for interpreting images and videos.
    *   **Speech Recognition** for converting spoken language into data.
*   From Data Understanding, we move upwards and sideways:
    *   To the left, **Data Access**: This is the domain of **Databases** (how to store and retrieve structured data efficiently) and **Information Retrieval** (finding relevant information from large collections, like web search).
    *   To the right, **Data Integration**: This is closely linked to **Data Warehousing**, which focuses on combining data from disparate sources for analysis.
*   Above Data Access and Data Integration, we have:
    *   **Data Analysis**: This is heavily influenced by **Machine Learning** (algorithms that allow computers to learn from data) and **Data Mining** (discovering patterns in large datasets).
*   And at the very top, encompassing both streams, is **Data Visualization**, which is a key part of **Human-Computer Interaction (HCI)**, focusing on how humans can effectively interact with and understand data.

Finally, the diagram points out that all of this enables **Many Applications!** This intricate interplay of disciplines is what makes the power of Big Data accessible and useful. It’s a truly interdisciplinary field.

<div class="page-break"></div>

### Basic Definitions

Let's revisit and solidify some basic definitions, which are crucial for our understanding.

*   **Database**: We've said it's a collection of related data. Simple, but fundamental.
*   **Data**: Again, known facts that can be recorded and have an implicit meaning. The raw material.
*   **Mini-world**: This term refers to some part of the real world about which data is stored in a database. It’s the specific context or domain that the database represents. For example, if we're building a university database, the mini-world includes students, courses, grades, and transcripts at that university. It’s the universe of discourse for our database.
*   **Database Management System (DBMS)**: This is a software package or system designed to facilitate the creation and maintenance of a computerized database. It's the engine that powers the database.
*   **Database system**: This term is a bit broader. It typically refers to the DBMS software *together with the data itself*. Sometimes, the applications that use the database are also considered part of the database system. So, it’s the whole environment.

These terms will be used throughout the course, so make sure you're comfortable with them.

<div class="page-break"></div>

### Impact of Databases and Database Technology

The impact of databases and database technology is truly pervasive. Let's consider various sectors:

*   In **Businesses**: Think about Banking (customer accounts, transactions), Insurance (policies, claims), Retail (inventory, sales, customer loyalty programs), Transportation (logistics, scheduling), Healthcare (patient records, billing), and Manufacturing (supply chain, production). Databases are the backbone of these operations.
*   In **Service industries**: This includes Financial services (stock trading, investment management), Real-estate (property listings, transactions), Legal services (case management, document storage), Electronic Commerce (online stores, payment processing), and Small businesses (customer relationship management, accounting).
*   In **Education**: Databases are used as resources for content and delivery, for student information systems, library catalogs, and online learning platforms.
*   **More recently**, we've seen their critical role in:
    *   **Social Networks**, as we discussed.
    *   **Environmental and Scientific Applications**, for managing research data, climate models, and biodiversity information.
    *   **Medicine and Genetics**, for genomics research, electronic health records, and drug discovery.
*   And finally, **Personalized applications**: These are increasingly based on smart mobile devices, using local or cloud-based databases to tailor experiences to individual users, from recommendation engines to personal assistants.

It's hard to imagine modern society functioning without database technology.

<div class="page-break"></div>

### A simplified architecture for a database system (Levels View)

This slide presents a simplified, layered architecture for a database system, focusing on different levels of abstraction.

At the top, we have the **View level**. This is what application programs see. A "view" is often a customized presentation of the data, tailored to the needs of a particular user or application. Importantly, views can also be used to hide information for security purposes. For example, a view might show instructor information but omit their salary for most users. The diagram shows multiple views (view 1, view 2, up to view n), indicating that different users can have different perspectives on the same underlying data.

Below the View level is the **Logical level**. This level describes what data is stored in the database and what relationships exist among that data. The example code snippet shows a conceptual definition of an `instructor` record, with fields like `ID`, `name`, `dept_name`, and `salary`, along with their data types (string, integer). This is part of the logical schema. It defines the structure of the database without specifying how it's physically stored.

At the bottom, we have the **Physical level**. This level describes *how* a record (e.g., an instructor record) is actually stored on the physical storage devices. It deals with file organization, indexing methods, and other low-level storage details. This level is typically hidden from application programmers and end-users.

The idea is that these layers provide data independence. Changes at the physical level (e.g., changing a storage structure for better performance) should ideally not require changes to the logical level or the views, and vice-versa for changes at the logical level (within certain limits) not affecting all views.

<div class="page-break"></div>

### A simplified architecture for a database system (Components View - Figure 1.1)

This diagram, labeled Figure 1.1, shows a simplified database system environment focusing on its main components and the flow of interaction.

At the top, we have **Users/Programmers**. These are the individuals or applications that interact with the database system.
They submit **Application Programs/Queries** to the system. A query might be a request for information, while an application program might perform a series of operations.

These requests go to the **DBMS Software**. The DBMS software itself can be thought of in two main parts here:
1.  **Software to Process Queries/Programs**: This component takes the incoming requests, parses them, optimizes them, and plans how to execute them.
2.  **Software to Access Stored Data**: This component is responsible for actually retrieving or modifying the data in the physical storage.

The Software to Access Stored Data interacts with the **Stored Data Layer** at the bottom. This layer consists of:
*   The **Stored Database Definition (Meta-Data)**: This is the schema, or the description of the database structure, data types, and constraints. It's "data about data."
*   The **Stored Database**: This is the actual data itself.

So, the flow is: Users submit requests, which are processed by the DBMS, which then uses the meta-data to understand how to access the actual stored data. This separation is key to how database systems operate.

<div class="page-break"></div>

### A simplified architecture for a database system (Detailed User Interaction View)

This is a more detailed architectural diagram, showing different types of users and the various components involved in processing their requests. It looks complex, but let's break it down by user type and follow the flow.

At the top, we see different categories of **Users**:
*   **DBA Staff** (Database Administrators): They interact with the system through:
    *   **DDL Statements** (Data Definition Language), which are used to define the database schema. These go to a **DDL Compiler**.
    *   **Privileged Commands**, for tasks like granting permissions or managing storage.
*   **Casual Users**: They typically use an **Interactive Query** interface. Their queries go to a **Query Compiler** and then to a **Query Optimizer**, which tries to find the most efficient way to execute the query.
*   **Application Programmers**: They write **Application Programs**. These programs often embed database manipulation commands. They go through a **Precompiler** (to separate database commands from the host programming language) and then a **DML Compiler** (Data Manipulation Language compiler).
*   **Parametric Users**: These users typically interact with pre-written applications, often involving **Host Language Programs** that are compiled by a **Host Language Compiler**, resulting in **Compiled Transactions**.

Now, let's look at the central processing pipeline:
The outputs from the DDL Compiler, Query Optimizer, DML Compiler, Compiled Transactions, and Privileged Commands all feed into, or interact with, the **Runtime Database Processor**. This is the core engine that executes the requests.

The Runtime Database Processor relies on several key components:
*   The **System Catalog/Data Dictionary**: This stores the meta-data (schema information). The DDL Compiler populates this, and the Runtime Processor consults it.
*   **Concurrency Control/Backup/Recovery Subsystems**: These are crucial for managing simultaneous access by multiple users and ensuring data integrity and recoverability in case of failures.
*   The **Stored Data Manager**: This component is responsible for the physical interaction with the **Stored Database**, handling input and output operations.

The dotted lines indicate flow of control or data. For example, the System Catalog is accessed by the Query Optimizer and the Runtime Database Processor. The Stored Data Manager reads from and writes to the Stored Database.

This diagram illustrates the sophisticated machinery involved in a modern DBMS to cater to different user needs and ensure reliable and efficient data management. The "Query and Transaction Execution" label at the bottom left emphasizes that this entire structure supports these core database operations.

<div class="page-break"></div>

### A simplified architecture for a database system (Query & Storage Manager View)

This diagram provides another perspective on the DBMS architecture, focusing on the interaction between the **Query Processor** and the **Storage Manager**.

Let's look at the top part, the **Query Processor**:
*   An **application program object code** (which might have been produced by a **compiler and linker**) or **DML queries** (Data Manipulation Language queries, like SQL SELECT statements) are fed into the system.
*   DML queries go to a **DML compiler and organizer**.
*   There's also a **DDL interpreter** for Data Definition Language statements (like CREATE TABLE).
*   All these feed into the **query evaluation engine**, which is the core component responsible for executing the queries.

Now, let's look at the bottom part, the **Storage Manager**. This manager is responsible for the interaction with the physical disk storage. It consists of several modules:
*   **Buffer Manager**: Manages the transfer of data between disk and main memory. It tries to keep frequently accessed data in memory buffers to speed up access.
*   **File Manager**: Manages the allocation of space on disk storage and the structure of the files that hold the data.
*   **Authorization and Integrity Manager**: Checks if users have the necessary permissions to access data and ensures that data modifications satisfy defined integrity constraints.
*   **Transaction Manager**: Ensures that transactions are executed atomically (all or nothing), consistently, in isolation from other transactions, and durably (once committed, changes are permanent).

The **query evaluation engine** from the Query Processor interacts with all these modules in the Storage Manager to get its job done. For example, it will request data blocks from the Buffer Manager, which might fetch them from disk via the File Manager. It will also interact with the Transaction Manager to ensure transactional properties.

Finally, at the very bottom, we have **Disk Storage**, which physically holds:
*   The **data** itself.
*   **Indices**, which are special data structures that speed up data retrieval.
*   The **data dictionary** (or system catalog), containing meta-data.
*   **Statistical data** about the data, which can be used by the query optimizer.

The DDL interpreter, for instance, would directly update the data dictionary. The Authorization and Integrity manager would consult the data dictionary. This diagram shows the crucial interplay between query processing logic and the lower-level storage management functions.

<div class="page-break"></div>

### What a DBMS Facilitates

So, having seen the architecture, what does a Database Management System actually *facilitate*? What are its key capabilities?

1.  It allows us to **Define** a particular database. This means specifying its data types (e.g., numbers, text, dates), its structures (e.g., tables and their columns), and any constraints that the data must adhere to (e.g., a student ID must be unique, or a grade must be within a certain range).
2.  It allows us to **Construct** the database. This involves loading the initial database contents onto a secondary storage medium, like a hard disk. This could be populating it with initial data or creating the empty structures.
3.  It allows us to **Manipulate** the database. This is a broad category that includes:
    *   **Retrieval**: This involves querying the database to extract information and generating reports.
    *   **Modification**: This includes inserting new data, deleting existing data, and updating current data.
    *   **Accessing the database through Web applications**: Modern DBMSs provide interfaces for web applications to interact with the database.
4.  And very importantly, it facilitates **Processing and sharing** by a set of concurrent users and application programs. Multiple users can access and modify the database simultaneously, and the DBMS ensures that all data remains valid and consistent, despite this concurrent access. This is a non-trivial task.

These four broad functions – Define, Construct, Manipulate, and Share/Process – encapsulate the core purpose of a DBMS.

<div class="page-break"></div>

### Other DBMS Functionalities

Beyond those core functions, a DBMS may additionally provide several other important functionalities:

*   **Protection or security measures**: These are designed to prevent unauthorized access to the data. This can include user authentication, authorization (granting specific permissions to users), and encryption.
*   **"Active" processing**: This refers to the ability of the DBMS to take internal actions on data automatically, often in response to certain events or conditions. This can be implemented using triggers or rules. For example, automatically reordering a product when its stock level falls below a threshold.
*   **Presentation and visualization of data**: Some DBMSs, or tools associated with them, offer capabilities for presenting data in user-friendly formats, such as charts, graphs, and dashboards, to help users understand the information.
*   **Maintenance of the database and associated programs**: This includes a range of activities that occur over the lifetime of the database application, such as performance tuning, schema evolution (modifying the database structure as requirements change), and managing software updates.

These additional functionalities enhance the utility, security, and manageability of the database system.

<div class="page-break"></div>

### Application Programs and DBMS

How do application programs interact with a DBMS?

Applications interact with a database primarily by generating two types of requests:

1.  **Queries**: These are requests that access different parts of the data and formulate the result of that request. For example, an application might generate a query to "find all students majoring in Computer Science who have a GPA greater than 3.5." The DBMS processes this query and returns the relevant data.
2.  **Transactions**: These are sequences of operations that may read some data and then "update" certain values, or generate new data and store that new data in the database. A key characteristic of a transaction is that it's treated as a single, indivisible unit of work. For example, transferring money from one bank account to another involves debiting one account and crediting another; both operations must succeed, or neither should, for the database to remain consistent.

So, applications use queries to retrieve information and transactions to modify the state of the database in a controlled manner.

<div class="page-break"></div>

### Example of a Database (with a Conceptual Data Model) - Mini-world and Entities

To make these concepts more tangible, let's consider an example of a database, and we'll start by thinking about its conceptual data model.

The **Mini-world** for our example will be part of a **UNIVERSITY environment**. This is the specific domain we want to model and store data about.

Within this UNIVERSITY mini-world, we can identify several key **entities**. Entities are the principal objects or concepts about which we want to store information. For our university example, some mini-world entities would be:
*   **STUDENTs**: Individuals enrolled in the university.
*   **COURSEs**: Academic subjects offered, like "Introduction to Databases."
*   **SECTIONs (of COURSES)**: Specific offerings of a course in a particular semester, taught by a particular instructor. For example, Section 001 of "CS101" in Fall 2023.
*   **(Academic) DEPARTMENTs**: Units within the university, like the "Computer Science Department" or the "History Department."
*   **INSTRUCTORs**: Faculty members who teach courses.

These are the fundamental "things" we are interested in tracking in our university database.

<div class="page-break"></div>

### Example of a Database (with a Conceptual Data Model) - Relationships

Now that we've identified the entities in our UNIVERSITY mini-world, let's consider some of the **relationships** between these entities. Relationships describe how entities are associated with each other.

Here are some mini-world relationships:
*   **SECTIONs *are of* specific COURSEs**: For example, Section 102 *is an offering of* the COURSE CS3320.
*   **STUDENTs *take* SECTIONs**: A student enrolls in, or takes, one or more sections of courses.
*   **COURSEs *have* prerequisite COURSEs**: For instance, "Data Structures" might *have* "Introduction to Programming" as a prerequisite.
*   **INSTRUCTORs *teach* SECTIONs**: An instructor is assigned to teach one or more sections.
*   **COURSEs *are offered by* DEPARTMENTs**: The "Database" course *is offered by* the "Computer Science" department.
*   **STUDENTs *major in* DEPARTMENTs**: A student declares a major, which is typically a department.

It's important to note that these entities and relationships are typically expressed formally in a **conceptual data model**. Two common ways to do this are using the **entity-relationship (ER) data model** or a **UML class model**. We will delve much deeper into these modeling techniques in Chapters 3 and 4. For now, this gives you a high-level idea of how we start thinking about the structure of a database.

---

All right, let's continue with the explanations for the remaining slides.

---

### Example of a Simple Database (Figure 1.2)

Now, let's look at Figure 1.2, which shows an example of a simple database that stores student and course information, reflecting the entities and relationships we just discussed. This slide presents the data in a tabular format, which is characteristic of relational databases.

We see four tables here:

1.  The **COURSE** table:
    *   It has columns for `Course_name` (like "Intro to Computer Science"), `Course_number` (like "CS1310"), `Credit_hours`, and the `Department` that offers the course. Each row represents a distinct course offered by the university.

2.  The **SECTION** table:
    *   This table gives details about specific offerings of courses. It has columns for `Section_identifier` (a unique ID for the section, like "85"), the `Course_number` it belongs to (linking it back to the COURSE table), the `Semester` and `Year` it's offered, and the `Instructor` teaching it. Each row is a specific section of a course.

3.  The **GRADE_REPORT** table:
    *   This table records the grades students receive in sections. It has `Student_number` (identifying the student), `Section_identifier` (identifying the section they took, linking to the SECTION table), and the `Grade` they received (like 'A', 'B', 'C'). Each row represents a grade for a student in a particular section.

4.  The **PREREQUISITE** table:
    *   This table defines prerequisite relationships between courses. It has `Course_number` (the course that has a prerequisite) and `Prerequisite_number` (the course that *is* the prerequisite). For example, one row might show that CS3380 (Database) has CS3320 (Data Structures) as a prerequisite.

By looking at these tables, you can start to see how information is organized and how relationships are represented (often by having common columns across tables, like `Course_number` appearing in both COURSE and SECTION). This is a very concrete example of what data in a database might look like.

<div class="page-break"></div>

### The relational model

This slide introduces **The relational model**, which is the most widely used model for databases today. The tables we just saw in the previous example are an illustration of this model.

The diagram highlights key terminology:
*   A table, in relational model terms, is formally called a **relation**.
*   The **Columns** of the table represent **attributes**, or properties of the entities. In the example table shown (which looks like an `INSTRUCTOR` or `EMPLOYEE` table), the columns are `ID`, `name`, `dept_name`, and `salary`.
*   The **Rows** of the table represent individual **tuples** or **records**. Each row corresponds to a single instance of an entity – in this case, a specific instructor with their ID, name, department, and salary.

The image on the right is of **E.F. "Ted" Codd**. He was a researcher at IBM who, in 1970, published the seminal paper that introduced the relational model of data. His work revolutionized database theory and practice, and he received the Turing Award (the "Nobel Prize of computing") for this contribution. The relational model provides a strong mathematical foundation for database design and manipulation.

So, when we talk about tables, rows, and columns, we are generally operating within the framework of the relational model.

<div class="page-break"></div>

### Main Characteristics of the Database Approach

Let's now discuss the main characteristics of using the database approach, as opposed to older file-processing systems. What makes it distinct and powerful?

1.  **Self-describing nature of a database system**:
    *   A DBMS stores not only the data itself but also the *description* of that data. This description is stored in a **DBMS catalog** or data dictionary. It includes information about the data structures (like table names and column names), data types (integer, string, etc.), and constraints (like uniqueness or value ranges).
    *   This description is called **meta-data**, which literally means "data about data."
    *   This self-describing nature is crucial because it allows the DBMS software to be general-purpose. The same DBMS software can work with many different database applications (e.g., a university database, a banking database, an inventory database) simply by accessing their respective catalogs to understand their structure.

2.  **Insulation between programs and data**:
    *   This is often called **program-data independence**.
    *   It means that you can change the structure of the data (e.g., add a new column to a table) or change how the data is stored physically (e.g., change the file organization or add an index) *without* necessarily having to change the programs that access that data (at least, not all of them, and ideally, most of them).
    *   This is a huge advantage. In older systems, if you changed the file format, you often had to rewrite all the programs that used that file. The DBMS provides a layer of abstraction that insulates programs from these kinds of changes.
    *   Abstract Data Types (ADTs) are an example of a concept that supports this kind of insulation, where the internal representation can change without affecting how the object is used.

These two characteristics – being self-describing and providing program-data independence – are fundamental to the flexibility and maintainability of database systems.

<div class="page-break"></div>

### Example of a Simplified Database Catalog (Figure 1.3)

This slide, Figure 1.3, shows an example of what a simplified database catalog might look like for the UNIVERSITY database we saw in Figure 1.2. Remember, the catalog stores meta-data.

We see two main tables in this catalog example:

1.  The **RELATIONS** table:
    *   This table lists all the relations (or tables) in our user database.
    *   It has columns for `Relation_name` (e.g., STUDENT, COURSE, SECTION, GRADE_REPORT, PREREQUISITE) and `No_of_columns` (the number of columns in each of those user tables).
    *   So, this part of the catalog tells the DBMS what tables exist and how many attributes each one has.

2.  The **COLUMNS** table:
    *   This table provides details about each column in each of the user tables.
    *   It has columns for `Column_name` (e.g., Name, Student_number, Class, Major, Course_name, Course_number, Prerequisite_number), the `Data_type` of that column (e.g., Character(30), Character(4), Integer(1), Major_type, Character(10), XXXXNNNN), and `Belongs_to_relation` (which user table this column is part of).
    *   The note at the bottom clarifies that `Major_type` is an enumerated type (a predefined list of values) and `XXXXNNNN` is a custom type format (four alpha characters followed by four digits).

This is a simplified view, of course. A real catalog would store much more information, such as details about keys, indexes, integrity constraints, user permissions, and so on. But this gives you the core idea: the DBMS uses this catalog to understand the structure and properties of the user's data.

<div class="page-break"></div>

### Main Characteristics of the Database Approach (continued)

Let's continue with more main characteristics of the database approach.

3.  **Data abstraction**:
    *   A **data model** (like the relational model, or the ER model we'll see later) is used to hide the details of data storage from users. Instead, it presents users with a **conceptual view** of the database. Users think in terms of entities, attributes, and relationships, not in terms of bits, bytes, and file pointers.
    *   As a result, programs refer to the constructs of the data model (e.g., tables and columns in the relational model) rather than the low-level data storage details. This simplifies application development and makes programs more resilient to changes in physical storage.

4.  **Support of multiple views of the data**:
    *   We touched on this earlier. Each user, or group of users, may see a different **view** of the database.
    *   A view describes **only** the data that is of interest to that particular user or is relevant to their task. It can be a subset of the data, or it can be derived data (e.g., an average).
    *   This allows for customization and also enhances security, as users only see what they are authorized to see. For example, a student might see their own grades but not the grades of other students. A registrar might see all student grades.

These characteristics contribute significantly to the usability, security, and maintainability of database systems.

<div class="page-break"></div>

### Main Characteristics of the Database Approach (continued)

And one more crucial characteristic:

5.  **Sharing of data and multi-user transaction processing**:
    *   Databases are inherently designed to be shared. They allow a set of **concurrent users** (multiple users at the same time) to both retrieve data from and update the database.
    *   To manage this concurrent access safely, the DBMS provides **concurrency control** mechanisms. These mechanisms guarantee that each transaction (a sequence of database operations treated as a single unit) is correctly executed or aborted (rolled back if it fails), even when multiple transactions are running simultaneously. This prevents problems like lost updates or inconsistent data.
    *   The **recovery subsystem** ensures that if a transaction completes successfully (commits), its effect is permanently recorded in the database, even if there's a subsequent system failure (like a power outage). If a transaction fails or is aborted, the recovery subsystem ensures that any partial changes it made are undone.
    *   **OLTP (Online Transaction Processing)** is a major part of many database applications. These are systems that handle a high volume of short, fast transactions, often from many concurrent users. Think of airline reservation systems, banking ATMs, or e-commerce checkouts. A key requirement for OLTP systems is the ability to allow hundreds, or even thousands, of concurrent transactions to execute per second, and the DBMS must manage this efficiently and reliably.

This ability to handle concurrent access and ensure data integrity underpins the utility of databases in most real-world applications.

<div class="page-break"></div>

### Database Users

Now, let's talk about the people who interact with databases. We can broadly divide database users into two main categories:

1.  Those who actually **use and control the database content**, and those who **design, develop, and maintain database applications**. These are often called **"Actors on the Scene."** They are directly involved with the database and its applications in their day-to-day work.
2.  Those who **design and develop the DBMS software itself** and related tools, and the **computer systems operators** who keep the hardware and software running. These are often called **"Workers Behind the Scene."** They build and maintain the infrastructure that the "actors on the scene" use.

Let's look at these categories in more detail.

<div class="page-break"></div>

### Database Users – Actors on the Scene

First, let's focus on the "Actors on the Scene." These are the users most directly visible when we think about a database application.

Two key roles here are:

1.  **Database Administrators (DBAs)**:
    *   DBAs are responsible for the overall management of the database system. Their responsibilities are wide-ranging and critical. They are responsible for:
        *   Authorizing access to the database (who can see what, who can do what).
        *   Coordinating and monitoring its use (how is the database being used, are there performance bottlenecks?).
        *   Acquiring necessary software and hardware resources.
        *   Controlling its use (enforcing policies, ensuring compliance).
        *   And monitoring the efficiency of operations (tuning performance, planning for capacity).
    *   The DBA is a central figure in any organization that relies on databases.

2.  **Database Designers**:
    *   Database designers are responsible for defining the content, the structure, the constraints, and the functions or transactions that will operate against the database.
    *   They translate the requirements of the users into a logical and physical database design.
    *   A crucial part of their job is to communicate effectively with the end-users to understand their needs and ensure that the database design accurately reflects those needs.

These two roles, DBA and Database Designer, are often distinct, especially in larger organizations, though in smaller settings, one person might wear both hats.

<div class="page-break"></div>

### Database End Users

Continuing with "Actors on the Scene," we have the **End-users**. These are the people who use the data for queries, reports, and, in some cases, to update the database content. End-users themselves can be categorized further:

1.  **Casual end-users**:
    *   These users access the database occasionally, when they need specific information. They might be managers who need ad-hoc reports, or researchers looking for particular data points. They often use sophisticated query languages or data analysis tools.

2.  **Naïve or Parametric end-users**:
    *   These users typically make up a large section of the end-user population.
    *   They interact with the database through pre-written, well-defined functions or "canned transactions." They don't write their own queries; they use interfaces provided by application programs.
    *   Examples include:
        *   Users of mobile apps – most interactions are through predefined buttons and forms.
        *   Bank-tellers or airline reservation clerks. They perform a limited set of operations (like check account balance, book a flight) repeatedly throughout their workday using a specific application interface.
        *   Social media users who post content or read information from websites are also, in a sense, parametric users interacting with the underlying database through the website's interface.

The distinction here is mainly about how they interact with the system – casual users have more flexibility and often more technical skill, while naïve/parametric users use predefined pathways.

<div class="page-break"></div>

### Database End Users (continued)

Let's look at a couple more categories of end-users:

3.  **Sophisticated end-users**:
    *   These users are thoroughly familiar with the system's capabilities and often have specific, complex data analysis needs.
    *   They include business analysts, scientists, and engineers.
    *   Many of them use specialized tools, such as statistical packages, data mining software, or modeling tools, that work closely with the stored database to perform complex analyses. They might write their own scripts or complex queries.

4.  **Stand-alone end-users**:
    *   These users mostly maintain personal databases using ready-to-use packaged applications. They are not typically interacting with a large, shared, organizational database.
    *   An example is a user of a tax preparation program (like TurboTax). The program creates and manages its own internal database to store the user's financial information for that tax year.
    *   Another example is a user who maintains a personal database of their photos and videos using software like Adobe Lightroom or Apple Photos. These applications manage a local database of metadata and image files.

So, the term "end-user" covers a wide spectrum of people with different needs and technical skills.

<div class="page-break"></div>

### Database Users – Actors on the Scene (continued)

Still within the "Actors on the Scene," we have another important group: **System analysts and application developers**. These are the people who build the applications that many end-users interact with.

*   **System Analysts**:
    *   They play a crucial role in bridging the gap between end-users and the technical implementation.
    *   They understand the user requirements of both naïve and sophisticated end-users.
    *   Based on these requirements, they design applications, including the "canned transactions" that naïve users will use, to meet those needs. They define the functionality and workflow of the applications.

*   **Application Programmers** (often called software developers or engineers):
    *   They take the specifications developed by the system analysts and implement them.
    *   They write the actual code for the applications, test them thoroughly, and debug any issues before the applications are deployed for end-users.

*   **Business Analysts**:
    *   This is an increasingly important role, especially with the rise of "Big Data."
    *   These individuals are skilled in analyzing vast amounts of business data and real-time data to derive insights that can inform better decision-making.
    *   Their work often relates to strategic planning, marketing effectiveness, advertising campaigns, operational efficiency, and so on. They use data to help the business achieve its goals.

These roles are essential for creating and maintaining the software systems that make databases useful to the broader organization.

<div class="page-break"></div>

### Database Users – Actors behind the Scene

Now let's turn to the "Workers Behind the Scene." These are the people who build and maintain the DBMS software and the underlying infrastructure, rather than the applications that use the DBMS.

1.  **System designers and implementors (of the DBMS itself)**:
    *   These are the software engineers who design and implement the DBMS packages – the database engines like Oracle, MySQL, SQL Server, PostgreSQL, etc.
    *   They develop the complex modules and interfaces that make up the DBMS (like the query optimizer, storage manager, concurrency control module).
    *   They also test and debug the DBMS software.
    *   A critical aspect of their work is ensuring that the DBMS can effectively interface with application programs, programming language compilers, the operating system, and other system components.

2.  **Tool developers**:
    *   These individuals design and implement software systems called "tools" that help in the database lifecycle.
    *   Examples of such tools include:
        *   Tools for modeling and designing databases (like ER diagramming tools).
        *   Performance monitoring tools.
        *   Prototyping tools.
        *   Test data generation tools.
        *   User interface creation tools.
        *   Simulation tools.
    *   These tools facilitate the building of database applications and allow users and developers to use the database more effectively.

3.  **Operators and maintenance personnel**:
    *   These are the people responsible for the day-to-day running and maintenance of the database system hardware and software environment.
    *   This includes tasks like installing and upgrading the DBMS software, monitoring system performance, performing backups, managing storage, and troubleshooting hardware or software problems. They keep the lights on, so to speak.

These "behind the scene" workers are crucial for providing the robust and reliable platform that the "actors on the scene" depend upon.

<div class="page-break"></div>

### Advantages of Using the Database Approach

We've touched on some of these already, but let's explicitly list the advantages of using the database approach, particularly when compared to older, traditional file-processing systems.

1.  **Controlling redundancy in data storage and in development and maintenance efforts**:
    *   In file systems, the same piece of information might be stored in multiple files, leading to redundancy. This wastes space and can lead to inconsistencies if data is updated in one place but not another. Databases aim to store each piece of data (ideally) only once, or in a controlled manner.
    *   This also reduces redundant development effort, as common data access routines can be centralized.
    *   A direct benefit of this is the **Sharing of data among multiple users** and applications, as everyone accesses the same, consistent data source.

2.  **Restricting unauthorized access to data**:
    *   DBMSs provide sophisticated security and authorization mechanisms. The DBA can grant specific privileges to users, controlling who can read, write, or modify data. This is much harder to enforce consistently in a simple file system. Only the DBA staff typically uses the most privileged commands and facilities.

3.  **Providing persistent storage for program Objects**:
    *   This is particularly relevant for object-oriented programming. Object-oriented DBMSs (or object-relational DBMSs) can make program objects persistent, meaning the objects and their state can survive beyond the execution of the program that created them. We'll explore this more in Chapter 12.

4.  **Providing storage structures (e.g., indexes) for efficient query processing**:
    *   DBMSs use sophisticated data structures, like B-trees and hash indexes, to speed up the retrieval of data. These are managed by the DBMS, often transparently to the application programmer. This is crucial for good performance, especially with large datasets. Chapter 17 will cover indexing in detail.

These are significant advantages that have made the database approach dominant.

<div class="page-break"></div>

### Advantages of Using the Database Approach (continued)

Let's continue with more advantages:

5.  **Providing optimization of queries for efficient processing**:
    *   When a user submits a query (especially in a declarative language like SQL, where you say *what* data you want, not *how* to get it), the DBMS has a query optimizer. The optimizer analyzes the query and considers various ways to execute it, choosing a plan that it estimates will be the most efficient. This is a complex task but can lead to huge performance gains.

6.  **Providing backup and recovery services**:
    *   DBMSs have mechanisms to regularly back up the database and to recover the database to a consistent state in case of failures (hardware crashes, software errors, power outages). This is essential for protecting valuable data.

7.  **Providing multiple interfaces to different classes of users**:
    *   As we saw, different users have different needs. DBMSs often support various interfaces:
        *   Query languages (like SQL) for technical users.
        *   Forms-based interfaces or graphical user interfaces (GUIs) for naïve users.
        *   Programming language APIs (Application Programming Interfaces) for application developers.

8.  **Representing complex relationships among data**:
    *   Databases are good at modeling and managing not just individual pieces of data, but also the complex ways in which different pieces of data are related to each other (e.g., students enroll in courses, courses are taught by instructors, departments offer courses).

9.  **Enforcing integrity constraints on the database**:
    *   DBMSs allow designers to define rules (integrity constraints) that the data must always satisfy. For example, a student ID must be unique, an order total must be positive, or a foreign key must refer to an existing primary key. The DBMS then automatically enforces these constraints, ensuring data quality.

10. **Drawing inferences and actions from the stored data using deductive and active rules and triggers**:
    *   Some advanced DBMSs support deductive capabilities (deriving new facts from existing ones) or active capabilities (using triggers to automatically execute actions when certain events or conditions occur in the database).

The combination of these advantages makes the database approach very powerful and versatile.

<div class="page-break"></div>

### Additional Implications of Using the Database Approach

Beyond the direct advantages, using the database approach has some other important implications for an organization:

1.  **Potential for enforcing standards**:
    *   When data is centralized and managed by a DBMS, it becomes easier to enforce organizational or industry standards.
    *   These **standards** can refer to:
        *   Data item names and definitions.
        *   Display formats for data.
        *   Screen layouts and report structures.
        *   The structure and content of meta-data.
        *   Web page layouts for data presentation.
    *   Enforcing standards improves consistency, interoperability, and understanding of data across the organization.

2.  **Reduced application development time**:
    *   Because the DBMS handles many low-level data management tasks (storage, indexing, concurrency, recovery), application developers can focus more on the business logic of their applications.
    *   Furthermore, once a database is established, the **incremental time to add each new application** that uses that data is often reduced, as the data and some access mechanisms are already in place. This promotes reusability.

These are significant organizational benefits.

<div class="page-break"></div>

### Additional Implications of Using the Database Approach (continued)

Let's look at a few more implications:

3.  **Flexibility to change data structures**:
    *   As business requirements evolve, the structure of the database may need to change (e.g., adding new types of information, modifying existing ones).
    *   The database approach, particularly with program-data independence, provides more flexibility to allow the **database structure to evolve as new requirements are defined**, often with less impact on existing applications than in traditional file systems.

4.  **Availability of current information**:
    *   Because data is typically centralized and updated in real-time (or near real-time), users and applications have access to the most current information.
    *   This is **extremely important for on-line transaction systems** such as e-commerce shopping sites, airline reservation systems, hotel booking systems, and car rental systems, where decisions are made based on the latest data.

5.  **Economies of scale**:
    *   By consolidating data and applications across departments into a centralized (or logically centralized) database system, organizations can achieve economies of scale.
    *   **Wasteful overlap of resources** (like redundant data storage or separate hardware for each department's files) and **personnel** (e.g., multiple people managing similar but separate data collections) can be avoided.
    *   This can lead to cost savings and more efficient use of IT resources.

These factors contribute to the overall business value derived from adopting a database approach.

<div class="page-break"></div>

### Historical Development of Database Technology

To appreciate where we are today, it's useful to look at the historical development of database technology.

1.  **Early database applications (mid-1960s to 1970s)**:
    *   The earliest commercially successful DBMSs were based on the **Hierarchical** and **Network** data models. These were introduced in the mid-1960s and dominated database processing during the 1970s.
    *   In the hierarchical model, data is organized in a tree-like structure. IBM's IMS (Information Management System) is a prime example and is still used in many legacy systems today.
    *   In the network model, data is organized in a more general graph structure, allowing more complex relationships than the hierarchical model.
    *   It's worth noting that a bulk of the worldwide database processing *still* occurs using these older models, particularly the hierarchical model with systems like IMS, due to the longevity of many critical mainframe applications.

2.  **Relational model-based systems (1970s onwards)**:
    *   As we mentioned, the **Relational model** was originally introduced by E.F. Codd in 1970.
    *   It was heavily researched and experimented with, notably within IBM Research and at several universities, throughout the 1970s.
    *   The first commercial **Relational DBMS Products** (like Oracle, DB2, Ingres) began to emerge in the early 1980s. The relational model quickly gained popularity due to its simplicity, mathematical rigor, and the power of declarative query languages like SQL. It has been the dominant model for decades.

This sets the stage for further developments.

<div class="page-break"></div>

### Historical Development of Database Technology (continued)

Let's continue with the historical timeline:

3.  **Object-oriented and emerging applications (late 1980s - 1990s onwards)**:
    *   **Object-Oriented Database Management Systems (OODBMSs)** were introduced in the late 1980s and early 1990s.
    *   They were designed to cater to the need for complex data processing in applications like Computer-Aided Design (CAD), Computer-Aided Manufacturing (CAM), and other engineering and scientific domains, where the relational model sometimes felt restrictive for representing complex objects and their behaviors.
    *   However, **their use has not taken off** as widely as initially anticipated, though they found niches in certain areas.
    *   In response, many **relational DBMSs have incorporated object database concepts** (like user-defined types, inheritance, and methods). This led to a new category called **object-relational DBMSs (ORDBMSs)**, which try to combine the best of both worlds.
    *   These **Extended relational systems** also added further capabilities to handle new data types, such as multimedia data (images, video), text documents, XML data, and other complex data types, making relational databases more versatile.

<div class="page-break"></div>

### Historical Development of Database Technology (continued)

And finally, a more recent phase in the historical development:

4.  **Data on the Web and e-commerce applications (late 1990s onwards)**:
    *   The rise of the World Wide Web brought new challenges and opportunities for data management.
    *   Initially, much of the **Web contained data in HTML** (Hypertext Markup Language), with links among pages. While good for presentation, HTML is not ideal for structured data exchange.
    *   The Web has given rise to a whole new set of applications, and **E-commerce**, in particular, has driven the need for robust database backends.
    *   New standards like **XML (eXtended Markup Language)** emerged to provide a more structured way to represent and exchange data on the web. We'll look at XML in Chapter 13.
    *   **Script programming languages** such as **PHP** and **JavaScript** became very popular for web development. These languages allow for the generation of **dynamic Web pages** – pages whose content is partially generated from a database at the time they are requested. Think of product listings on an e-commerce site or personalized news feeds. We'll touch on this in Chapter 11.
    *   Importantly, these scripting languages also **allow database updates through Web pages**, enabling users to interact with and modify database content via a web browser (e.g., submitting an order, updating a profile).

This evolution continues today with cloud databases, NoSQL systems, and the challenges of Big Data.

<div class="page-break"></div>

### Extending Database Capabilities (1)

The field of databases is not static; new functionality is constantly being added to DBMSs to meet evolving needs. Here are some areas where database capabilities are being extended:

*   **Scientific applications**: Databases are being enhanced to better support the needs of scientific disciplines like physics, chemistry, biology, and genetics, which often deal with very large, complex, and specialized datasets (e.g., experimental results, molecular structures, genomic sequences).
*   **Spatial data**: There's a growing need to manage spatial information for applications related to weather, earth and atmospheric sciences, astronomy, urban planning, and location-based services. This involves specialized data types (like points, lines, polygons) and spatial query operations (like "find all restaurants within 1 mile of this location").
*   **XML (eXtensible Markup Language)**: As mentioned, XML has become a standard for data representation and exchange. Many DBMSs now provide native support for storing, querying, and managing XML data.
*   **Image storage and management**: Storing and efficiently retrieving large collections of images, along with their metadata, is a growing requirement.
*   **Audio and video data management**: Similar to images, managing large multimedia libraries of audio and video content presents unique challenges.
*   **Time series and historical data management**: Many applications need to store and analyze data that changes over time (e.g., stock prices, sensor readings, patient medical history). This requires support for temporal data types and queries.

The development of these capabilities often gives rise to **new research and development** in areas like:
*   Incorporating new, complex data types into the DBMS.
*   Handling complex data structures beyond simple tables.
*   Defining new operations specific to these data types.
*   Devising new storage and indexing schemes optimized for these new kinds of data.

The goal is to make DBMSs more versatile and powerful for a wider range of applications.

<div class="page-break"></div>

### When not to use a DBMS

While DBMSs offer many advantages, they are not always the best solution for every data management problem. There are situations when *not* to use a DBMS.

First, let's consider the **main inhibitors (or costs) of using a DBMS**:
*   **High initial investment**: Commercial DBMS software can be expensive to license. There might also be a need for additional, more powerful hardware to run the DBMS effectively.
*   **Overhead**: A general-purpose DBMS provides many features – generality, security, concurrency control, recovery, integrity functions. All these features come with some processing overhead. For very simple tasks, this overhead might be unnecessary.

So, **when a DBMS may be unnecessary**:
*   If the database and the applications that use it are **simple, well-defined, and not expected to change** much over time. For example, a small, personal address book might be adequately managed with a simple spreadsheet or text file.
*   If **access to data by multiple users is not required**. If only a single user or a single application needs to access the data, the complexities of concurrency control and multi-user security might be overkill.

And, **when a DBMS may be infeasible**:
*   In **embedded systems** where resources (like memory and storage) are severely limited. A full-scale, general-purpose DBMS may simply not fit or may consume too many resources. In such cases, more lightweight data management solutions might be used.

It's important to weigh the benefits against the costs and complexity.

<div class="page-break"></div>

### When not to use a DBMS (continued)

Let's consider a few more scenarios where a full-fledged, traditional DBMS might not be the best fit, or where "no DBMS may suffice":

*   If there are **stringent real-time requirements** that may not be met because of DBMS overhead.
    *   For example, in telephone switching systems or high-frequency trading systems, the response time requirements are extremely tight (microseconds or milliseconds). The overhead introduced by a general-purpose DBMS for transaction management, logging, etc., might be too high. These systems often use highly specialized, in-memory data management techniques.
*   If the database system is **not able to handle the complexity of data because of modeling limitations** of the chosen DBMS.
    *   For example, in some complex genome and protein databases, the relationships and data structures might be so intricate that a traditional relational DBMS struggles to represent them efficiently or naturally. Specialized bioinformatics data management systems might be more appropriate.
*   If the database users need **special operations not supported by the DBMS**.
    *   For example, Geographic Information Systems (GIS) often require specialized spatial analysis operations (like overlaying maps, calculating distances along a network) that are not standard in general-purpose DBMSs (though many are adding such capabilities). Similarly, some location-based services might need very specific types of proximity queries or real-time tracking features.

In these cases, specialized systems, custom-built solutions, or even simpler file-based approaches might be more appropriate than a traditional, general-purpose DBMS. The key is to choose the right tool for the job.

<div class="page-break"></div>

### Chapter Summary

So, to summarize what we've covered in this introductory chapter:

*   We discussed the **Types of databases and database applications**, from traditional numeric/textual ones to more recent multimedia, GIS, and Big Data applications.
*   We established some **Basic definitions** for key terms like data, database, DBMS, and mini-world.
*   We looked at **Typical DBMS functionality**, including defining, constructing, manipulating, and sharing data.
*   We went through an **Example of a database (UNIVERSITY)** to illustrate entities, relationships, and how data might be structured in tables.
*   We highlighted the **Main characteristics of the database Approach**, such as its self-describing nature, program-data independence, data abstraction, support for multiple views, and multi-user transaction processing.
*   We categorized the different **Types of database users**, from DBAs and designers to various kinds of end-users and the "workers behind the scene."
*   We listed the numerous **Advantages of using the database approach**, such as controlling redundancy, restricting unauthorized access, and providing efficient query processing.
*   We took a brief look at the **Historical development of database technology**, from early hierarchical and network models to the relational model and more recent trends.
*   We touched upon how database capabilities are being **Extended** to handle new data types and application areas.
*   And finally, we considered **When not to use databases**, acknowledging that they are not a universal solution.

This chapter has laid the foundation for our exploration of database systems. I hope it has given you a good overview of the field and its importance. Are there any initial questions before we conclude this lecture?

---