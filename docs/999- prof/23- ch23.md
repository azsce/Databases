---
title: "23. Distributed Database Concepts"
---

#

## Distributed Database Concepts
Okay, I understand. I will now generate the professor's lecture script for the first half of the provided slides, focusing on clear explanations, a professional tone, and adherence to all the SQL-specific and formatting guidelines.

Here's the script for the first part:

---

Good morning, everyone. Today, we'll be diving into Chapter 23, which focuses on **Distributed Database Concepts**. This is a fascinating area that builds upon many of the foundational database principles we've discussed and extends them into more complex, real-world scenarios.

---
<div class="page-break"></div>

### Introduction

Alright, let's start with an **Introduction** to the core ideas.

*   When we talk about a `distributed computing system`, we're essentially looking at a setup where, instead of one single, powerful computer doing all the work, we have several distinct processing sites or, as we often call them, *nodes*. These nodes aren't isolated; they are interconnected by a computer network, allowing them to communicate and collaborate.
    *   The key here is cooperation. These nodes work together to perform certain tasks. Think of it like a team project – each member (or node) contributes to the overall goal.
    *   A significant advantage is the ability to partition a large, complex task into smaller, more manageable sub-tasks. These smaller tasks can then be distributed among the nodes, often leading to more efficient problem-solving.

*   Now, closely related to this, and very relevant in today's data-driven world, are `big data technologies`.
    *   These technologies are, in many ways, a fusion of distributed computing principles and traditional database technologies. They are designed to handle the massive volumes of data that modern applications generate and consume.
    *   A primary application of big data technologies is in mining these vast amounts of data – extracting valuable insights, patterns, and knowledge that would be impossible to uncover with older, more centralized approaches.

---
<div class="page-break"></div>

### 23.1 Distributed Database Concepts

So, let's refine our understanding of what specifically constitutes a `distributed database`.

*   What are the essential ingredients?
    *   First, there's the physical aspect: you need a **connection of database nodes over a computer network**. This is the infrastructure that allows different parts of the database to reside in different locations but still communicate.
    *   Crucially, there's a **logical interrelation of these connected databases**. This means that even though the data might be physically spread out, it's all part of one cohesive, logical database. From a user's perspective, it should ideally appear as a single database.
    *   And it's important to acknowledge the **possible absence of homogeneity among connected nodes**. This means the different sites or nodes might not all be running the same hardware, the same operating system, or even the same database software. This heterogeneity adds complexity but also flexibility.

*   Managing all of this is the `Distributed Database Management System`, or `DDBMS`.
    *   This is the **software system that manages a distributed database**. It’s the brain that makes all the distributed pieces work together, handling data distribution, query processing, concurrency control, and recovery across the network.

---

Now, let's continue with some more **Distributed Database Concepts**.

*   When we talk about the networks connecting these sites, we often distinguish between a `Local Area Network` or LAN, and a `Long-haul` or `Wide Area Network`, WAN.
    *   A LAN typically connects sites within a relatively small geographical area, like a single building or campus. **Hubs or cables** are common connection methods here.
    *   A WAN, on the other hand, spans larger distances, connecting sites across cities, countries, or even continents. For this, you'd see **telephone lines, dedicated cables, wireless links, or satellite connections**.

*   The **network topology** is also a key concept. This defines the communication path – how the different sites are interconnected. Is it a star, a ring, a mesh? The topology affects performance, reliability, and cost.

*   A very important goal in DDBMS design is `transparency`.
    *   Transparency essentially means **hiding implementation details from the end user**. The user interacts with the distributed database as if it were a single, centralized system, without needing to know where data is physically stored or how operations are executed across different sites. This greatly simplifies application development and user interaction.

---
<div class="page-break"></div>

### Transparency

Let's delve deeper into the different **Types of transparency** that a DDBMS aims to provide. This is a cornerstone of making distributed systems usable.

*   First, we have `Data organization transparency`.
    *   This type of transparency **hides how data is logically structured or physically stored**. Users can interact with the data – query it, update it – without needing to know the underlying details of its organization. For example, they don't need to be aware if the data is stored in tables, if those tables are partitioned, or how those partitions are distributed across different storage devices or nodes.

*   Next is `Location transparency`. This is perhaps one of the most intuitive types.
    *   With location transparency, **users can access data without knowing its physical location**. They don't need to specify which server or node stores a particular piece of data. The DDBMS handles the routing of requests to the correct site. Imagine querying an employee database; you just ask for the employee's record, not "the employee record stored on server X in the London office."

*   Then there's `Naming transparency`.
    *   This **ensures that a unique name identifies each data item, regardless of its location**. This is crucial for avoiding name conflicts when you have data spread across different sites, which might have been developed or managed independently before being integrated into the distributed system. The DDBMS provides a global naming scheme.

---

Continuing with our discussion on **Transparency**:

*   Another important type is `Replication transparency`.
    *   This **hides the fact that data may be replicated**, meaning stored in multiple locations. For users, they see and interact with what appears to be a single copy of the data, even if, behind the scenes, multiple copies exist. These copies might be there for improving fault tolerance – if one site goes down, a copy is still available – or for enhancing performance by allowing users to access a local copy.

*   And we also have `Fragmentation transparency`.
    *   This **conceals how data is split (or fragmented) across different nodes**. A large table, for instance, might be broken down into smaller pieces, or fragments, which are then stored at different sites. Users, however, still see the complete table.
    *   There are two main types of fragmentation:
        *   `Horizontal fragmentation`: Here, **rows of a table are distributed across different nodes**. For example, records of employees in different regions (say, North America, Europe, Asia) might be stored on servers located in those respective regions. Each fragment contains a subset of the rows but all the columns.
        *   `Vertical fragmentation`: In this case, **columns of a table are distributed**. For instance, for an employee table, employee names and job titles might be stored in one node, while their salary and performance review data are stored in another. Each fragment contains a subset of the columns but all the rows (or at least, the primary key to link them back).

---

Let's look at a couple more **Types of transparency**:

*   `Design transparency`.
    *   This aims to **hide the complexities of how the distributed system was designed**.
    *   Essentially, **users and applications do not need to know about the specific partitioning strategies or replication schemes** that have been implemented. They work with the logical view of the data, and the DDBMS takes care of mapping this to the physical, distributed reality.

*   And finally, `Execution transparency`.
    *   This **conceals the details of how a distributed operation is executed across multiple nodes**.
    *   A good **example** is a distributed transaction. A transaction might involve updates to data stored on several different servers. With execution transparency, this distributed transaction appears to the user or application as a single, atomic operation, even if it involves a complex coordination of activities across multiple servers. The DDBMS manages the two-phase commit or similar protocols to ensure atomicity.

---
<div class="page-break"></div>

### Distributed Databases

Now, let's visualize how data might be distributed and replicated. If you look at **Figure 23.1** on the slide, it gives us a good example of `Data distribution and replication among distributed databases`.

We see a central `Chicago (Headquarters)` node. This node seems to hold `All EMPLOYEES`, `All PROJECTS`, and all `WORKS_ON` data, perhaps acting as a master repository or a central point for certain global operations.

Then, this headquarters is connected via a `Communications Network` to several other regional sites:
*   There's a `San Francisco` node. The slide indicates that this site might store data for `EMPLOYEES` specifically in `San Francisco and Los Angeles`. The `PROJECTS` data here is for San Francisco, and `WORKS_ON` data pertains to San Francisco employees. This suggests some level of data localization.
*   Similarly, there's a `Los Angeles` node. It stores `EMPLOYEES` data for Los Angeles, and its `PROJECTS` data covers both Los Angeles and San Francisco (perhaps due to collaborations). The `WORKS_ON` data is for Los Angeles employees.
*   Moving to the East Coast, we have a `New York` node. This node stores `EMPLOYEES` data for New York. Interestingly, it seems to have access to `All PROJECTS` (perhaps replicated from headquarters or a global view) and `WORKS_ON` data for New York employees.
*   And finally, an `Atlanta` node, which stores `EMPLOYEES`, `PROJECTS`, and `WORKS_ON` data all specific to Atlanta.

This figure illustrates a few key things:
1.  **Data Distribution:** Different pieces of data are stored at different sites, often based on geographical relevance or operational needs.
2.  **Data Replication:** Some data, like "All PROJECTS" at the New York site and "All" data at Headquarters, might be replicated. The San Francisco and Los Angeles sites also show some overlap in project data.
3.  **Fragmentation:** The `EMPLOYEES` table, for example, is horizontally fragmented by location.

This setup aims to improve local access times, data availability, and potentially allows for autonomous operation of the sites to some extent, while still being part of a larger, interconnected system.

---
<div class="page-break"></div>

### Availability and Reliability

Two critical quality attributes for any database system, especially distributed ones, are **Availability and Reliability**.

*   Let's define `Availability` first.
    *   This refers to the **probability that the system is continuously available during a specified time interval**. So, if we say a system has 99.999% availability (often called "five nines"), it means it's expected to be operational for that percentage of the time. For distributed systems, high availability is often a key design goal, as the failure of one node shouldn't necessarily bring down the entire system.

*   Then there's `Reliability`.
    *   Reliability is the **probability that the system is running (meaning, not down) at a *certain point in time***. While related to availability, reliability focuses more on the system's ability to function correctly without failure when it's supposed to be operational.

*   It's important to note that **both availability and reliability are directly related to how the system handles faults, errors, and failures**. A fault is a defect, an error is an incorrect system state due to a fault, and a failure is when the system doesn't deliver its service as specified.

---

Given the importance of **Availability and Reliability**, distributed systems employ various **Fault-tolerant approaches**:

*   One of the most common is `Replication`.
    *   As we've touched upon, **multiple copies of data are maintained across different nodes**. This ensures continuity of service if one node (or the data on it) fails, as operations can be redirected to a node with a replica.

*   Another strategy is `Redundancy`.
    *   This involves having **extra components – like servers, network links, or power supplies – that can take over if the primary components fail**. This isn't just about data, but the infrastructure itself.

*   `Consensus Protocols` are also crucial.
    *   These are **algorithms, such as Paxos or Raft, that ensure consistency across the distributed system, even in the presence of failures**. For example, if multiple nodes need to agree on a value or the order of operations, consensus protocols help them reach that agreement reliably.

*   And finally, `Check pointing & Recovery` mechanisms are vital.
    *   **Periodic snapshots (checkpoints) of the database state are taken**. If a crash occurs, the system can be restored to the last consistent checkpoint, and then logs can be used to roll forward any committed transactions that happened after the checkpoint, allowing for quick recovery.

These approaches work together to make distributed systems resilient to various kinds of failures.

---
<div class="page-break"></div>

### Scalability and Partition Tolerance

Two more critical "-ilities" for distributed systems are **Scalability and Partition Tolerance**.

*   Let's start with `Horizontal scalability`.
    *   This refers to **expanding the system's capacity by adding more nodes to the distributed system**. If your workload increases, you can simply add more servers to the cluster. This is often preferred because it can, in theory, scale out almost indefinitely and can be more cost-effective than trying to make a single machine ever more powerful.

*   In contrast, there's `Vertical scalability`.
    *   This means **expanding the capacity of the *individual nodes*** – for example, by adding more CPU, RAM, or faster disks to an existing server. While this can provide performance boosts, there are physical and cost limits to how much you can scale up a single machine.

*   Then we have `Partition tolerance`. This is a fundamental concept, especially highlighted by the CAP theorem, which we might touch upon later.
    *   Partition tolerance means the **system should have the capacity to continue operating even if the network experiences a partition**. A network partition is a failure where communication between different groups of nodes is lost. A partition-tolerant system will continue to function in some capacity within each partition, though there might be tradeoffs in terms of consistency or availability of certain operations across partitions.

These characteristics are key design considerations when building robust and adaptable distributed database systems.

---
<div class="page-break"></div>

### Autonomy

Another important dimension in distributed databases is **Autonomy**, which refers to the independence of the individual nodes or sites within the distributed system.

*   **Autonomy determines the extent to which individual nodes can operate independently** of the central control or other nodes. There are different types or aspects of autonomy:

*   First, `Design autonomy`.
    *   This refers to the **independence of data model usage and transaction management techniques among nodes**. For example, one site in a heterogeneous distributed database might use a relational model and two-phase locking, while another site uses an object-oriented model with a different concurrency control mechanism. Design autonomy allows existing, diverse systems to be integrated.

*   Next is `Communication autonomy`.
    *   This **determines the extent to which each node can decide *whether* and *how* to share information with other nodes**. A node might choose not to participate in certain global operations or to restrict the data it shares.

*   And then there's `Execution autonomy`.
    *   This means the **independence of users at a local site to act as they please on their local data**, potentially without interference or coordination from a central administrator or other sites. They can execute local operations, and the local DBMS manages these operations.

The degree of autonomy can vary significantly from one DDBMS to another, impacting how the system is managed and how applications interact with it.

---
<div class="page-break"></div>

### Advantages of Distributed Databases

So, why go through all the complexity of building and managing distributed databases? There are several compelling **Advantages**.

*   One major benefit is **improved ease and flexibility of application development**.
    *   This is particularly true for organizations that are geographically dispersed. **Development can happen at geographically dispersed sites**, with each site focusing on its local data needs while still being part of a coherent global system. This allows for more modular and responsive application development.

*   Another key advantage is **increased availability**.
    *   Because data and processing can be distributed and often replicated, the failure of a single site or a communication link doesn't necessarily bring the entire system down. You can **isolate faults to their site of origin**, allowing other parts of the system to continue operating.

*   **Improved performance** is also a common goal.
    *   Through `data localization`, data can be stored closer to the users who access it most frequently. This reduces network latency and can significantly speed up query responses for local operations.

*   And finally, **easier expansion via scalability**.
    *   As we discussed with horizontal scalability, it's generally **easier to scale a distributed system by adding more nodes than it is to scale up a single, centralized system** beyond a certain point. This allows the system to grow more gracefully with increasing demands.

These advantages often outweigh the complexities involved, making distributed databases a preferred solution for many modern, large-scale applications.

---
<div class="page-break"></div>

### 23.2 Data Fragmentation, Replication, and Allocation Techniques for Distributed Database Design

Now, let's move into Section 23.2, which discusses specific **Data Fragmentation, Replication, and Allocation Techniques for Distributed Database Design**. These are the core strategies for deciding *how* and *where* data is stored in a DDBMS.

*   First, let's talk about `Fragments`.
    *   Fragments are essentially **logical units of the database**. When we decide to break up a large relation (or table), the resulting pieces are called fragments.

*   A primary way to create fragments is through `Horizontal fragmentation`, which is also commonly known as `sharding` in many modern systems.
    *   A **horizontal fragment or shard of a relation is a subset of the tuples (or rows) in that relation**. So, you're dividing the table row-wise.
    *   This fragmentation **can be specified by a condition on one or more attributes or by some other method**. For example, for an `EMPLOYEE` table, you might create fragments based on the `Department_ID` attribute (all employees in Dept 1 go to fragment A, Dept 2 to fragment B) or based on `Location` (all employees in New York go to one fragment, all in London to another).
    *   The idea is to **group rows to create subsets of tuples**, and critically, **each subset typically has a certain logical meaning** that makes this fragmentation sensible from an application or access pattern perspective.

---

Continuing with **Data Fragmentation**:

*   The counterpart to horizontal fragmentation is `Vertical fragmentation`.
    *   This technique **divides a relation vertically by columns**.
    *   So, it **keeps only certain attributes of the relation** in each fragment. For example, an `Employee` relation might be vertically fragmented into one fragment containing `EmpID`, `Name`, and `Address`, and another fragment containing `EmpID`, `Salary`, and `PerformanceRating`. The `EmpID` (or some primary key) would typically be included in all vertical fragments to allow the original tuple to be reconstructed.

*   When we talk about `Complete horizontal fragmentation`, it means that if you take all the horizontal fragments of a relation and apply the `UNION` operation to them, you should be able to reconstruct the original, complete relation without any loss of data or duplication (assuming disjoint fragments).

*   Similarly, for `Complete vertical fragmentation`, if you apply an `OUTER UNION` or, more commonly, a `FULL OUTER JOIN` operation (using the common key attribute) to all the vertical fragments, you should be able to reconstruct the original relation. The join is necessary here because each fragment contains different columns for the same set of original entities.

---

Let's look at a few more aspects of **Data Fragmentation**:

*   It's also possible to have `Mixed (hybrid) fragmentation`.
    *   This is simply a **combination of horizontal and vertical fragmentations**. For example, you might first horizontally fragment an `Orders` table by region, and then, for each regional fragment, you might vertically fragment it to separate order details from shipping information.

*   The `Fragmentation schema` is an important concept.
    *   This **defines the set of all fragments that includes all attributes and all tuples in the original database**. It's the blueprint that describes how the entire database has been broken down into these smaller, manageable pieces.

*   And finally, the `Allocation schema`.
    *   Once you have your fragments, you need to decide where to store them. The allocation schema **describes the allocation of these fragments to the various nodes of the Distributed Database System (DDBS)**. A fragment might be stored at one node, or it might be replicated and stored at multiple nodes.

The design of the fragmentation and allocation schemas is a critical task, as it heavily influences the performance, availability, and manageability of the distributed database.

---
<div class="page-break"></div>

Okay, here is the professor's lecture script for the second half of the slides.

---

### Data Replication and Allocation

Alright, let's continue our discussion with **Data Replication and Allocation**. We've defined fragments, and now we need to decide how many copies of each fragment to keep and where to place them.

*   One extreme approach is a `Fully replicated distributed database`.
    *   In this scenario, there's **replication of the *whole* database at *every site* in the distributed system**. Every node has a complete copy of all the data.
    *   The primary advantage here is that it **improves availability remarkably**. If any number of sites fail (short of all of them), the data is still accessible from the remaining sites. Read operations can also be very fast, as they can always be satisfied locally.
    *   However, there's a significant downside: **update operations can be very slow**. Why? Because every update has to be propagated and applied to every single copy across all sites to maintain consistency. This can create a huge overhead and potential bottlenecks.

*   At the other end of the spectrum is `Nonredundant allocation` (which essentially means no replication).
    *   Here, **each fragment is stored at exactly one site**. There are no copies.
    *   This simplifies update operations (as there's only one place to update), but it reduces availability (if the site holding a fragment fails, that fragment becomes unavailable) and might lead to higher access times if users frequently need data stored at remote sites.

---

### Data Replication and Allocation (cont'd.)

Most real-world systems opt for a middle ground, which brings us to **Partial replication**.

*   With `Partial replication`, **some fragments are replicated, and others are not**. You might choose to replicate frequently read, critical data, while less critical or frequently updated data might not be replicated, or replicated less extensively.
    *   This strategy is **defined by a replication schema**, which specifies which fragments are replicated and to which sites.

*   This leads us to the broader concept of `Data allocation` (also known as `data distribution`).
    *   This involves deciding, for **each fragment, which particular site or sites in the distributed system it will be assigned to**.
    *   The **choices made here depend heavily on the performance and availability goals of the system**. For instance:
        *   If high availability for a particular fragment is paramount, it will be replicated across multiple sites, preferably in different fault domains.
        *   If fast local access for a specific user group is the goal, the fragment they use most will be allocated to a site geographically or network-wise close to them.
        *   If minimizing update overhead is crucial for a volatile fragment, it might be stored at only one site or very few sites.

Finding the right balance in data replication and allocation is a complex optimization problem, often involving trade-offs between read performance, write performance, storage costs, and availability.

---
<div class="page-break"></div>

### Example of Fragmentation, Allocation, and Replication

Let's consider an **Example of Fragmentation, Allocation, and Replication** to make these concepts more concrete.

*   Imagine a **company with three computer sites**, perhaps corresponding to three major departments or geographical locations – say, Sales, Manufacturing, and Engineering.
    *   It's logical to place **one primary computer site for each department**.
    *   We would **expect frequent access by employees working in a particular department to the data most relevant to that department and to the projects controlled by that department**. For example, sales staff will frequently access customer and order data; manufacturing will access production schedules and inventory.

*   The slide mentions to **see Figures 23.2 and 23.3 in the textbook for a detailed example of fragmentation among these three sites**. While we don't have those figures directly on this slide, we can imagine how it might work.
    *   For instance, the `Customer` table might be horizontally fragmented by sales region, and each regional fragment allocated to the sales office site for that region.
    *   The `Product_Design` table might be primarily allocated to the Engineering site.
    *   Shared data, like `Employee_Master_List`, might be replicated at all three sites for high availability and quick local lookups.
    *   The key is to align the data fragmentation and allocation with how the business operates and how data is actually used by different parts of the organization.

---
<div class="page-break"></div>

### 23.3 Overview of Concurrency Control and Recovery in Distributed Databases

Now we shift our focus to Section 23.3: **Overview of Concurrency Control and Recovery in Distributed Databases**. These are challenging areas in DDBMS because the distribution of data and processing introduces new complexities compared to centralized systems.

*   For concurrency control (managing simultaneous access by multiple users) and recovery (restoring the database to a consistent state after failures), **numerous new problems arise in a distributed DBMS environment**.

    *   One major challenge is **dealing with multiple copies of data items** due to replication.
        *   If a piece of data is replicated on several sites, and multiple transactions try to update it concurrently, how do we ensure all copies are updated correctly and consistently? The **concurrency control method is responsible for maintaining consistency among these copies**. This often involves more complex protocols than in centralized systems.

    *   Another significant issue is the **failure of individual sites**.
        *   The **DDBMS should ideally continue to operate with its running sites, if possible, when one or more individual sites fail**. This is known as graceful degradation. How the system handles queries and transactions that require data from a failed site is a critical aspect of recovery.

---

Continuing our overview of **Concurrency Control and Recovery in Distributed Databases**:

*   We also have to contend with the **failure of communication links**.
    *   The **system must be able to deal with the failure of one or more of the communication links that connect the sites**. If sites cannot communicate, it can lead to network partitions, making it difficult to coordinate distributed transactions or ensure consistency of replicated data.

*   `Distributed commit` is another area fraught with challenges.
    *   **Problems can arise with committing a transaction that is accessing or updating databases stored on multiple sites, especially if some sites fail *during* the commit process**. For example, what if one site successfully commits its part of the transaction, but another site fails before it can commit? This could lead to an inconsistent database state.
    *   The **two-phase commit protocol (2PC)**, which we've likely discussed in the context of transaction management (as mentioned, see Section 21.6 in the text for a refresher), is often used to deal with this problem by ensuring that all participating sites either all commit or all abort the transaction.

*   And then there's `Distributed deadlock`.
    *   **Deadlock may occur among several sites**. For example, a transaction T1 at site A might be waiting for a resource held by transaction T2 at site B, while T2 at site B is waiting for a resource held by T1 at site A. Detecting and resolving such deadlocks is more complex in a distributed environment than in a centralized one, so **techniques for dealing with deadlocks must be extended to take this into account**. This might involve global wait-for graphs or timeout mechanisms.

---
<div class="page-break"></div>

### Distributed Concurrency Control Based on a Distinguished Copy of a Data Item

Let's look at some specific approaches to **Distributed Concurrency Control Based on a Distinguished Copy of a Data Item**.

The core idea here is that to manage concurrent access to replicated data, we designate one particular copy of each data item as the `distinguished copy` (sometimes called the primary copy or master copy). Locks and synchronization for that data item are then managed through this distinguished copy.

*   One such method is the `Primary site technique`.
    *   In this straightforward approach, a **single primary site is designated to be the coordinator site for *all* database items**. This means all locks for all data items across the entire distributed database are kept at this one primary site. Consequently, **all requests for locking or unlocking any data item, regardless of where other copies might reside, are sent to this central primary site**.
    *   There are, however, significant **disadvantages**.
        *   One is that **all locking requests are funneled to a single site**. This can easily **overload that site and cause a system bottleneck**, especially in a busy system.
        *   A second, and perhaps more critical, disadvantage is that **failure of this single primary site paralyzes the entire system** from a concurrency control perspective. No new locks can be acquired or released, effectively halting most transactional work.

---

To address some of the issues with the simple primary site technique, we can enhance it. This leads to the `Primary site with backup site` approach.

*   **Primary site with backup site**:
    *   This approach directly **addresses the second disadvantage (single point of failure) of the basic primary site method by designating a second site to be a backup site**.
    *   All **locking information is maintained at *both* the primary and the backup sites**. So, every lock request and grant is recorded in two places.
    *   **In case of primary site failure, the backup site takes over as the new primary site**, and typically, a new backup site would then be chosen or promoted from other available nodes. This provides fault tolerance for the locking mechanism.
*   However, this also has its **Disadvantages**.
    *   It **slows down the process of acquiring locks**. Why? Because **all lock requests and the granting of locks must be recorded at both the primary and the backup sites *before* a response is sent back to the requesting transaction**. This introduces extra communication overhead and latency for every lock operation.

---

Another variation on using a distinguished copy is the `Primary copy method`.

*   **Primary copy method**:
    *   This method **attempts to distribute the load of lock coordination among various sites by having the distinguished copies of *different* data items stored at *different* sites**.
    *   So, instead of one central site managing all locks, Site A might hold the primary copy (and thus manage locks) for data items X and Y, while Site B holds the primary copy for data items P and Q. This helps to avoid the single bottleneck of the basic primary site technique.
    *   With this approach, **failure of one site only affects any transactions that are accessing locks on items whose primary copies reside at *that specific failed site***. Other transactions, which need locks for data items whose primary copies are on operational sites, are not affected and can continue. This improves overall system resilience compared to the single primary site approach.

---
<div class="page-break"></div>

### Distributed Concurrency Control Based on Voting

An alternative to distinguished copy methods is **Distributed Concurrency Control Based on Voting**.

*   The `Voting method` works differently:
    *   There is **no single distinguished copy** for a data item. All replicated copies are, in a sense, equal for locking purposes.
    *   When a transaction wants to lock a data item, **lock requests are sent to *all* sites that contain a copy** of that data item.
    *   **Each site that holds a copy maintains its own local lock** on that copy.
    *   The key rule is: **If a transaction that requests a lock is granted that lock by a *majority* of the sites holding copies, then it effectively holds the lock on *all* copies**. This is often referred to as a quorum-based approach.
    *   What if it doesn't get a majority? **If a transaction does not receive a majority of votes granting it a lock within a certain time-out period, it cancels its request and informs all sites of the cancellation**. It will likely have to retry later.
    *   A **time-out period applies** to prevent indefinite waiting.
    *   The main **Disadvantage** of the voting method is that it typically **results in higher message traffic among sites** because every lock request (and release) for a replicated item involves communicating with multiple sites to gather votes. However, it can be more resilient to individual site failures than some primary copy schemes, as long as a majority of sites remain operational.

---
<div class="page-break"></div>

### Distributed Recovery

Let's touch upon **Distributed Recovery**. As with concurrency control, recovery in a distributed environment is more complex.

*   One fundamental issue is that it's **difficult to determine reliably whether a remote site is truly down or just slow to respond, without exchanging numerous messages with other sites**. Is it a site failure, or a network partition, or just extreme network congestion? This uncertainty complicates recovery protocols.

*   We've already mentioned `Distributed commit` and its challenges.
    *   A core principle here is: **When a transaction is updating data at several sites, it *cannot* commit until it is certain that its effect on *every* participating site cannot be lost**. This is the "all-or-nothing" property of atomicity, extended to a distributed setting.
    *   The **two-phase commit (2PC) protocol is often used to ensure this correctness**. We'll briefly outline its phases again shortly, as it's fundamental to distributed transaction management.

---
<div class="page-break"></div>

### 23.4 Overview of Transaction Management in Distributed Databases

This leads us to Section 23.4, an **Overview of Transaction Management in Distributed Databases**.

*   A key component is the `Global transaction manager (GTM)`.
    *   The GTM **supports distributed transactions** – transactions that access or modify data at multiple sites.
    *   This is often a **role temporarily assumed by the DDBMS at the site where the transaction originated**. This site's GTM then acts as the coordinator for that specific distributed transaction.
    *   Its primary function is to **coordinate the execution of the transaction with the local transaction managers (LTMs) at all the multiple sites involved** in the transaction.
*   The GTM also **passes database operations (like read, write, update) and associated information (like data item names, new values) to the appropriate concurrency controller** at each site.
    *   The local **concurrency controller is then responsible for the acquisition and release of locks** (or other concurrency control mechanisms like timestamps) for the data items at its site, according to the chosen concurrency control protocol.

---
<div class="page-break"></div>

### Commit Protocols

Let's elaborate on the **Commit Protocols** that are essential for ensuring atomicity in distributed transactions.

*   The **Two-phase commit (2PC)** protocol is the most well-known.
    *   The **Coordinator** (typically the Global Transaction Manager at the originating site) plays a crucial role. It **maintains information needed for recovery** in case of failures during the commit process. This information is logged.
    *   This is **in addition to the local recovery managers** at each participating site, which also log their parts of the transaction.
    *   Briefly, 2PC involves:
        1.  **Phase 1 (Voting/Prepare Phase):** The coordinator sends a "prepare to commit" message to all participants. Each participant checks if it can commit its part of the transaction. If yes, it writes a "prepared" record to its log (making its changes durable locally) and sends a "vote-commit" message back. If no, it sends "vote-abort".
        2.  **Phase 2 (Decision/Commit-Abort Phase):** If the coordinator receives "vote-commit" from *all* participants, it decides to commit, logs this decision, and sends a "commit" message to all. If it receives even one "vote-abort" or a timeout, it decides to abort, logs this, and sends an "abort" message. Participants then act on the coordinator's decision (commit or abort) and send an acknowledgment.

*   There's also a **Three-phase commit (3PC)** protocol.
    *   3PC aims to be non-blocking in more failure scenarios compared to 2PC (which can block if the coordinator fails after participants have voted to commit but before it sends the final commit/abort decision).
    *   It essentially **divides the second commit phase of 2PC into two subphases**:
        *   A `prepare-to-commit` subphase where the coordinator, after receiving all "vote-commit" messages, sends a "prepare-to-commit" message to all participants. Participants acknowledge this and are now in a state where they know all others have also agreed to prepare.
        *   The final `commit` subphase is then similar to the commit part of 2PC, where the coordinator sends the final "commit" message. The extra phase helps reduce the window of uncertainty if the coordinator fails.

While 3PC is more robust in theory, 2PC is more commonly implemented due to its relative simplicity and lower overhead in the absence of failures.

---

### 23.5 Query Processing and Optimization in Distributed Databases

Let's now turn our attention to Section 23.5: **Query Processing and Optimization in Distributed Databases**. Just as with transactions, handling queries in a distributed environment presents unique challenges and opportunities for optimization.

*   There are several **Stages of a distributed database query** process:

    *   First, **Query mapping**.
        *   The initial query, often expressed in a high-level language like SQL by the user or application, **refers to the global conceptual schema**. The user sees a unified view of the database, unaware of how or where the data is fragmented or replicated. The DDBMS must first parse this query and validate it against this global schema.

    *   Next is **Localization**.
        *   This crucial stage **maps the distributed query (which operates on the global schema) to a set of separate sub-queries that can operate on the individual fragments** stored at different sites. This involves determining which fragments are relevant to the query and how to access them. For example, if a query asks for all employees with a salary greater than $50,000, and the `EMPLOYEE` table is horizontally fragmented by department, the localization step will generate sub-queries for each department fragment, filtering for the salary condition.

    *   Then comes **Global query optimization**.
        *   Once the query is localized into operations on fragments, the DDBMS needs to find an efficient execution strategy. This involves considering different ways to execute the sub-queries, transfer data between sites, and combine intermediate results. A **strategy is selected from a list of candidate strategies**, often based on a cost model that estimates execution time, data transfer costs, and resource utilization.

    *   Finally, there's **Local query optimization**.
        *   After a global strategy is chosen and sub-queries are dispatched to individual sites, each site performs its own local optimization on the sub-query it received. This is **common to all sites in the DDB** and uses traditional query optimization techniques suited for the local DBMS and data structures at that site.

---

Continuing with **Query Processing and Optimization in Distributed Databases**:

*   A major factor in distributed query performance is the **Data transfer costs of distributed query processing**.
    *   This refers to the **cost of transferring intermediate result files and the final result files between sites** over the network. Network communication is often much slower than local disk access or CPU processing, so minimizing data transfer is usually a primary goal of distributed query optimization.
    *   For instance, if you need to join a large table at Site A with another large table at Site B, simply shipping one entire table to the other site for the join might be very inefficient.

*   Therefore, the main **Optimization criterion** is often **reducing the amount of data transfer**. The query optimizer will explore various strategies, such as performing operations locally as much as possible, choosing optimal join orders, and using techniques like semijoins (which we'll discuss next) to reduce the size of data that needs to be moved across the network.

---

Let's elaborate on a specific technique for optimizing distributed queries: **Distributed query processing using semijoin**.

*   The semijoin operation is particularly useful in distributed environments because it **reduces the number of tuples in a relation *before* transferring it to another site** for a join operation.
*   Here's how it generally works:
    *   Suppose you want to join relation R (at Site A) with relation S (at Site B) on a common attribute.
    *   Instead of sending all of R to Site B or all of S to Site A, you can first **send *only* the joining column values of one relation (say, relation R) to the site where the other relation S is located (Site B)**.
    *   At Site B, these joining column values from R are used to select only those tuples from S that can actually participate in the join. This effectively "pre-filters" S.
    *   Then, these matching tuples from S (or just their joining attributes and any other required result attributes from S) are **shipped back to the original site (Site A)**, where the final join with R can be performed. Or, alternatively, the reduced R (if S was used to filter R) is sent to S.
*   The semijoin can be a very **efficient solution to minimizing data transfer**, especially if the selectivity of the join condition is high (meaning only a small fraction of tuples actually match). By transferring only the necessary joining column values and then the reduced relation, we can avoid sending large amounts of non-matching data over the network.

---

Let's consider **Query and update decomposition** further.

*   A key goal of transparency is that the **user can specify a query as if the DBMS were centralized**.
    *   This ideal is achieved **if full distribution, fragmentation, and replication transparency are supported** by the DDBMS. The user simply writes their query against the global schema, and the system figures out how to execute it across the distributed fragments and replicas.

*   The **Query decomposition module** plays a vital role in this.
    *   It **breaks up a user's query (against the global schema) into a set of subqueries that can be executed at the individual sites** where the relevant data fragments are stored.
    *   Crucially, a **strategy for combining the results from these subqueries must also be generated** by the optimizer to produce the final answer to the user's original query.

*   To perform this decomposition and localization correctly, the DDBMS relies on its catalog (or metadata). The **catalog stores information like the attribute list for each fragment and/or the guard condition** (the condition used for horizontal fragmentation, e.g., `Location = 'New York'`). This information allows the query decomposition module to determine which fragments are relevant to a given query.

---
<div class="page-break"></div>

### 23.6 Types of Distributed Database Systems

Now we move to Section 23.6, which discusses **Types of Distributed Database Systems (DDBMSs)**. Not all DDBMSs are created equal; they can vary along several dimensions.

*   There are key **Factors that influence the types of DDBMSs**:

    *   One is the **Degree of homogeneity of the DDBMS software** across the different sites.
        *   A `Homogeneous` DDBMS is one where all sites run the same DDBMS software (e.g., all sites run Oracle, or all sites run a specific version of a research DDBMS). This simplifies design and interoperability.
        *   A `Heterogeneous` DDBMS is one where different sites may run different DBMS software (e.g., one site runs Oracle, another runs SQL Server, and a third runs a legacy system). This is much more complex to manage but often arises when integrating pre-existing, disparate database systems.

    *   Another factor is the **Degree of local autonomy** that each site possesses.
        *   At one extreme, there might be **No local autonomy**, where all sites are tightly controlled by a central administration, and local sites cannot operate independently.
        *   At the other extreme, a `Multidatabase system` often has **full local autonomy**, where each participating database system remains independent in its operations and administration, and only agrees to participate in a federation for certain shared tasks.

*   This brings us to the concept of a `Federated database system (FDBS)`.
    *   In an FDBS, multiple autonomous database systems (which could be heterogeneous) are interconnected to allow users to access data from them in an integrated way. A key characteristic is that a **global view or schema of the federation of databases is shared by the applications**, providing a unified interface to the underlying disparate systems. However, the individual component databases typically retain a high degree of autonomy.

---
<div class="page-break"></div>

### Classification of Distributed Databases

**Figure 23.6** on the slide presents a useful **Classification of distributed databases** along three dimensions: Distribution, Autonomy, and Heterogeneity.

Let's interpret this 3D diagram:
*   The **vertical axis represents `Distribution`**. Higher up means more distribution.
*   The **horizontal axis pointing to the right represents `Autonomy`**. Further to the right means more local autonomy for the sites.
*   The **axis pointing out towards us (depth) represents `Heterogeneity`**. Further out means the component systems are more different from each other (e.g., different data models, different DBMS software).

The diagram then plots four general types of systems:
*   **Point A** is at the origin: low distribution, low autonomy, low heterogeneity. This represents **Traditional centralized database systems**.
*   **Point B** is high on Distribution, but potentially lower on Autonomy and Heterogeneity (though this can vary). This represents **Pure distributed database systems**, designed from the ground up as a single logical DDBMS, often homogeneous.
*   **Point C** shows moderate to high levels of Heterogeneity and Autonomy, along with some degree of Distribution. This is indicative of **Federated database systems (FDBS)**. Here, pre-existing, autonomous, and possibly different database systems are linked together.
*   **Point D** is also in the region of high Heterogeneity and Autonomy, and high Distribution. This represents **Multidatabase systems or peer-to-peer database systems**. These are often characterized by a collection of independent databases that agree to cooperate, perhaps without a centrally defined global schema, relying more on negotiation and shared interfaces.

This classification helps us understand that "distributed database" is not a monolithic term but covers a wide spectrum of system architectures.

---
<div class="page-break"></div>

### Types of Distributed Database Systems (cont'd.)

Let's delve into some of the specific issues that arise, particularly with **Federated database management systems (FDBSs)**, due to their inherent heterogeneity and autonomy.

*   **Federated database management systems issues**:
    *   One common issue is **Differences in data models**. For example, one component DBS might be relational, another object-oriented, and a third might be a hierarchical or network database. The FDBS must provide a way to map between these different models to present a unified view.
    *   There can also be **Differences in constraints**. The types of integrity constraints supported (e.g., primary keys, foreign keys, check constraints) and how they are enforced can vary significantly between different DBMSs.
    *   **Differences in query languages** are also a major challenge. If one system uses SQL, another uses OQL (Object Query Language), and a third has a proprietary query language, the FDBS needs a mechanism to translate queries from a common federated language into the native languages of the component systems, and to integrate the results.

*   Perhaps the most difficult challenge is `Semantic heterogeneity`.
    *   This refers to **differences in meaning, interpretation, and intended use of the same or related data** across different component databases. For example, an attribute named `SALARY` might mean annual salary in one database and monthly salary in another. Or, `PRODUCT_ID` might refer to different product coding schemes in different systems. Resolving semantic conflicts requires a deep understanding of the data in each system and often involves complex schema integration and data transformation logic.

---

Continuing with **Types of Distributed Database Systems**, specifically focusing on the implications of autonomy:

*   `Design autonomy`, which we mentioned earlier, allows individual component databases in a federation to have control over their own design. This means each component system can define the following parameters independently:
    *   **The universe of discourse from which its data is drawn**. For example, one database might focus on financial data, while another focuses on manufacturing data.
    *   Its own **representation and naming** conventions for data elements.
    *   The local **understanding, meaning, and subjective interpretation of its data**. This is where semantic heterogeneity often arises.
    *   Its own set of **transaction and policy constraints**, such as security rules, access controls, and business rules.
    *   How it performs **derivation of summaries** or aggregated data.

This design autonomy is powerful for allowing existing systems to join a federation without requiring them to be completely re-engineered, but it places a significant burden on the FDBS to mediate these differences.

---

Let's look at other forms of autonomy relevant in federated or multidatabase systems:

*   `Communication autonomy`:
    *   This implies that a component DBS can **decide whether to communicate with another component DBS** or with the federated layer. It might choose to process certain requests and reject others, or it might be unavailable for communication at certain times (e.g., during local maintenance).

*   `Execution autonomy`:
    *   This means a component DBS can **execute its local operations without interference from external operations initiated by other component DBSs** or the federated layer. For example, local transactions have priority.
    *   It also includes the **ability for the local DBS to decide the order of execution** for operations, including those that are part of a global federated transaction. This can complicate global concurrency control and transaction management.

*   `Association autonomy`:
    *   This refers to the ability of a component DBS to **decide whether and how much to share its functionality and resources** with the federation. It might expose only a subset of its data or services, and it can decide to join or leave the federation.

These various forms of autonomy underscore the "loosely coupled" nature of many federated and multidatabase systems, contrasting with the more "tightly coupled" approach of homogeneous DDBMSs.

---
<div class="page-break"></div>

### 23.7 Distributed Database Architectures

Let's move to Section 23.7 and discuss **Distributed Database Architectures**.

*   First, it's useful to distinguish between **Parallel versus distributed architectures**.
    *   While both involve multiple processors, *parallel database systems* typically use multiple processors that are tightly coupled, often within a single machine or a closely connected cluster, sharing resources like memory or disk. The goal is high-speed query processing through parallelism.
    *   *Distributed database systems*, as we've been discussing, involve processors (at different sites or nodes) that are more loosely coupled, connected via a network, and may not share memory or disk directly. The emphasis is on data distribution, site autonomy, and availability across geographically dispersed locations.
    *   Of course, there's overlap, and many modern systems combine aspects of both.

*   When we look at systems with multiple processors, there are several common **Types of multiprocessor system architectures**:
    *   `Shared memory (tightly coupled)`: All processors share access to a common main memory. This allows for very fast inter-processor communication but can become a bottleneck as the number of processors increases.
    *   `Shared disk (loosely coupled)`: Each processor has its own private memory, but all processors share access to a common set of disks. Data is exchanged by reading and writing to these shared disks. This offers better scalability than shared memory for some workloads.
    *   `Shared-nothing`: This is the most scalable architecture for distributed systems. Each processor (or node) has its own private memory *and* its own private disk(s). There is no direct sharing of resources. All communication and data exchange between nodes happens by passing messages over the network. This architecture forms the basis for many massively parallel processing (MPP) databases and distributed NoSQL systems.

---
<div class="page-break"></div>

### Database System Architectures

**Figure 23.7** on the slide illustrates some of these different **Database System Architectures**.

*   **Part (a)** of the figure shows a `Shared-nothing architecture`.
    *   You can see multiple `Computer System` boxes (Computer System 1, Computer System 2, ..., Computer System n). Each box contains its own `CPU`, its own `Memory`, and its own `DB` (representing its local disk storage for database data). These systems are interconnected via a `Switch` or network. This is the classic shared-nothing model – no shared memory, no shared disk between the independent systems.

*   **Part (b)** depicts `A networked architecture with a centralized database at one of the sites`.
    *   Here, we have a `Central Site` (labeled Chicago), which hosts the actual databases `DB1` and `DB2`. Other sites (`San Francisco`, `Los Angeles`, `New York`, `Atlanta`) are connected to this central site via a `Communications Network`. These remote sites don't have their own local databases in this model; they access the data stored centrally. This is more like a client-server architecture with a centralized database, rather than a truly distributed database where data itself is partitioned and stored at multiple sites.

*   **Part (c)** illustrates `A truly distributed database architecture`.
    *   In this diagram, we see multiple sites (`Site 1`, `Site 2`, `Site 3`, `Site 4`, `Site 5`), each with its own local database (represented by the cylinder next to each site). These sites are all interconnected via a `Communications Network`. This represents a scenario where data can be fragmented and allocated across these different sites, and the DDBMS manages the distributed data and query processing. This is more aligned with the shared-nothing principles applied to a geographically distributed environment.

---
<div class="page-break"></div>

### General Architecture of Pure Distributed Databases

Let's consider the **General Architecture of Pure Distributed Databases**, often assuming a homogeneous environment where the DDBMS is designed as a single, cohesive system spread across multiple sites. Key components typically include:

*   A `Global query compiler`.
    *   This component takes a query written against the global conceptual schema. It **references the global conceptual schema, usually stored in a global system catalog, to verify the query (e.g., check table and attribute names) and to impose any defined constraints** or business rules that apply at the global level.

*   A `Global query optimizer`.
    *   After the query is compiled and validated, the global query optimizer takes over. Its job is to **generate an optimized execution plan, which often involves decomposing the global query into optimized local queries that will be sent to the individual sites** holding the relevant data fragments. This optimizer considers data distribution, replication, network costs, and available operations at each site to find the most efficient plan.

*   A `Global transaction manager (GTM)`.
    *   We've discussed this before. The GTM **coordinates the execution of distributed transactions across multiple sites, working in conjunction with the local transaction managers (LTMs)** at each participating site. It's responsible for initiating commit protocols like 2PC to ensure atomicity.

These global components work together above the local DBMS components at each site to provide the illusion of a single, centralized database system to the users.

---
<div class="page-break"></div>

### Schema Architecture of Distributed Databases

**Figure 23.8** illustrates a common **Schema Architecture of Distributed Databases**, often referred to as the ANSI-SPARC architecture extended for distributed systems. This shows different levels of schemas:

*   At the top, we have the **User** interacting through an `External View` (or external schema). Each user or group of users might have their own customized view of the database, showing only the data they are authorized to see and in a format that is convenient for them.
*   Below the external views is the `Global Conceptual Schema (GCS)`. This is the logical description of the entire distributed database, as if it were centralized. It defines all the entities, attributes, and relationships in the database, independent of how or where the data is physically stored or fragmented. This is the schema against which users and applications typically write their queries.
*   The GCS maps to one or more `Local Conceptual Schemas (LCS)`. If the DDBMS is homogeneous, each site might have an LCS that is essentially a description of the portion of the GCS that is relevant to that site (e.g., the fragments stored there). In a heterogeneous DDBMS, the LCS might be the conceptual schema of the local database in its native data model. The dashed lines between LCSs suggest they are part of a distributed whole.
*   Below each LCS is a `Local Internal Schema (LIS)`. This describes the physical storage structures, indexes, access paths, and data layout for the data stored at that particular site. It's specific to the local DBMS and hardware at that site.
*   Finally, at the bottom, we have the `Stored Data` at each site (`Site 1`, `Site 2 to n-1`, `Site n`), which is managed according to its LIS.

This layered schema architecture provides data independence (changes at lower levels don't necessarily affect higher levels) and helps manage the complexity of the distributed environment.

---
<div class="page-break"></div>

### Federated Database Schema Architecture

Now let's look at a slightly more complex schema architecture often found in federated systems, as shown in **Figure 23.9: The five-level schema architecture in a federated database system (FDBS)**. Because FDBSs integrate autonomous and often heterogeneous databases, they need more layers to handle the integration and mapping.

The five levels, typically from bottom to top, are:

1.  **Local Schema**: This is at the very bottom, within each `Component DBS`. It's the native schema (conceptual, internal) of the individual, pre-existing database system in its own data model and language.
2.  **Component Schema**: This is derived from the local schema but expressed in a common data model used by the FDBS for integration purposes. It describes the data that a component DBS is willing to share with the federation.
3.  **Export Schema**: Each component DBS may define one or more export schemas. An export schema represents a subset of its component schema that it makes available to the federation or to specific federated schemas. This allows a component DBS to control what data it exposes.
4.  **Federated Schema**: This is where the integration happens. A federated schema is created by integrating multiple export schemas from different component DBSs. It provides a unified, global view over the data from these disparate sources. There might be multiple federated schemas, each tailored for different user communities or applications. The ellipses (...) indicate that there can be multiple schemas at these levels.
5.  **External Schema**: At the top, users and applications interact with the FDBS through external schemas, which are views defined on a federated schema. This provides a customized and potentially simplified view of the integrated data.

This multi-layered architecture is essential for handling the autonomy and heterogeneity inherent in FDBS environments, allowing for controlled sharing and integration of data while respecting the independence of the component systems.

---
<div class="page-break"></div>

### An Overview of Three-Tier Client/Server Architecture

**Figure 23.10** provides **An Overview of Three-Tier Client/Server Architecture**. While not exclusively a distributed *database* architecture, it's a very common architectural pattern for modern applications that often *use* distributed databases or access databases over a network. The key idea is the separation of concerns into three logical tiers.

*   The slide notes that the **Division of DBMS functionality among the three tiers can vary**, but a typical setup is:

    1.  **Client Tier (Presentation Tier)**:
        *   This is what the user interacts with directly. It's responsible for the user interface and presentation logic.
        *   Examples include a `Web browser` rendering `HTML` and running `JavaScript`, or a desktop application built with `Visual Basic` or similar UI tools.
        *   It takes user input and displays results.

    2.  **Application Server Tier (Business Logic Tier / Middle Tier)**:
        *   This tier contains the application's business logic and processing rules.
        *   It receives requests from the client tier, processes them according to the business rules (e.g., validating data, performing calculations, orchestrating workflows), and then interacts with the database tier to fetch or store data.
        *   This tier might host `Application programs` written in languages like `JAVA`, `C/C++`, `C#`, etc.
        *   Communication between the Client and Application Server often uses protocols like `HTTP Protocol`.

    3.  **Database Server Tier (Data Tier / Backend Tier)**:
        *   This tier is responsible for data storage and management. It houses the actual DBMS and the database.
        *   It handles `Query and transaction processing`, `Database access`, and ensures data integrity and security.
        *   It executes `SQL` queries, stored procedures (`PSM` - Persistent Stored Modules), and may handle `XML` data, etc.
        *   The Application Server communicates with the Database Server using database connectivity protocols and APIs like `ODBC` (Open Database Connectivity), `JDBC` (Java Database Connectivity), `SQL/CLI` (Call Level Interface), or `SQLJ`.

This three-tier architecture promotes modularity, scalability (you can scale each tier independently), and maintainability. The database itself in the third tier could be centralized or distributed.

---
<div class="page-break"></div>

### 23.8 Distributed Catalog Management

Finally, let's discuss Section 23.8: **Distributed Catalog Management**. The system catalog (or data dictionary) stores metadata – information about the database schemas, tables, columns, fragments, replicas, users, access rights, etc. In a distributed database, managing this catalog itself becomes a distributed problem.

There are several strategies for managing the catalog in a DDBMS:

*   **Centralized catalogs**:
    *   In this simple approach, the **entire catalog is stored at one single site**.
    *   The main advantage is that it's **easy to implement** and manage, as there's only one place to update and query metadata.
    *   However, it suffers from the same drawbacks as any centralized component: it can become a performance bottleneck (all metadata requests go to one site) and a single point of failure (if the catalog site goes down, the entire DDBMS might become inoperable because it can't access schema information).

*   **Fully replicated catalogs**:
    *   Here, **identical copies of the complete catalog are present at each site** in the distributed system.
    *   This **results in faster reads** of metadata, as any site can access the catalog information locally. It also improves availability, as the catalog is still accessible even if some sites fail.
    *   The downside is that updates to the catalog (e.g., creating a new table, adding a fragment) become expensive, as the update must be propagated to all copies at all sites, similar to fully replicated data.

*   **Partially replicated catalogs**:
    *   This is often a more practical compromise. A common approach is that **each site maintains complete catalog information on the data that is stored *locally* at that site**.
    *   Additionally, sites might cache frequently accessed portions of the catalog related to remote data. For truly global information or to find data whose location isn't known locally, a site might need to query a master catalog site or use a distributed directory lookup mechanism. This approach tries to balance local access speed with the overhead of replication and consistency.

The choice of catalog management strategy depends on factors like the size of the DDBMS, the frequency of schema changes, performance requirements for metadata access, and desired availability.

---
<div class="page-break"></div>

### 23.9 Summary

To **Summarize** our discussion on Distributed Database Concepts from Chapter 23:

*   We defined the **Distributed database concept**, involving logically interrelated data stored across interconnected computer sites.
*   We emphasized the importance of various forms of **Transparency**, such as:
    *   `Distribution transparency` (including location and naming transparency)
    *   `Fragmentation transparency` (hiding how data is split horizontally or vertically)
    *   `Replication transparency` (hiding the existence of multiple data copies)
*   We looked at key **Design issues**, including:
    *   Techniques for **Horizontal and vertical fragmentation** of data.
    *   Strategies for data replication and allocation.
*   We had an overview of **Concurrency control and recovery techniques** tailored for distributed environments, such as distributed locking, voting, and two-phase commit.
*   We touched upon **Query processing** in DDBMSs, including decomposition, localization, and optimization with a focus on minimizing data transfer.
*   And finally, we discussed the **Categorization of DDBMSs** based on homogeneity, autonomy, leading to types like homogeneous DDBMS, federated database systems, and their associated schema architectures and challenges.

Distributed database systems are complex but essential for handling the demands of modern, large-scale, and geographically dispersed applications. Understanding these core concepts is crucial for anyone working with such systems.

That concludes Chapter 23. Thank you.